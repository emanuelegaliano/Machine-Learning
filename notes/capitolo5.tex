\chapter{Design, valutazione e scelta dei parametri di un algoritmo di ML}

\section{Momento e gradiente}
Una variante dell'algoritmo di discesa del gradiente, ovvero \textbf{la discesa del gradiente con momento}, modifica la nostra formula del gradiente nel seguente modo:

$$
\vartheta_j^\text{new} \leftarrow \vartheta_j^\text{old} - \alpha z^\text{new}
$$

$$
z^\text{new} \leftarrow \beta z^\text{old} + \frac{\partial J(\vartheta^\text{old})}{\partial \vartheta^\text{old}} 
$$

l'idea è quella di mantenere in $z$ una \textbf{media dei gradienti recenti}, in modo da mantenere coerenza durante la convergenza: valori incoerenti del gradiente sono \textbf{rumore}, e questo \textbf{può causare zig-zag e rallentamenti nella convergenza}. Tenendo uno storico dei valori del gradiente (usando il \textbf{momentum} $\beta$ come parametro che stabilisce l'influenza dei vecchi valori su quelli nuovi), diventa possibile \textbf{annullare parzialmente o totalmente l'effetto del rumore}. Accelera anche la convergenza stessa, \textbf{rinforzando i passi coerenti}. Si accorcia quindi il training, e si migliorano le performance a parità di epoche. Osservazioni statistiche consigliano l'utilizzo di $\beta = 0.90$ per ottenere risultati migliori.


\section{Design di Algoritmi}
Progettare bene un algoritmo di ML permette di ottenere migliori risultati in meno tempo e con meno risorse computazionali. Si pensi al cercare di inserire a casaccio i parametri di un modello, senza una logica precisa: si rischia di perdere tempo, se invece si pensa a come variare i parametri in maniera sistematica, per capire l'impatto di ciascuno di essi sul modello si potrebbe ottenere un modello migliore in meno tempo.

\subsection{Strategie di miglioramento}
Supponiamo di aver implementato la regressione lineare con regolarizzazione, ma gli errori su nuovi campioni sono comunque troppo alti. Potremmo pensare ad alcune strategie per migliorare il modello:
\begin{description}
	\item[Aumentare i campioni di Training.] Non è sempre una scelta utile, anche perché se il modello non funziona su nuovi campioni significa che il problema (molto probabilmente) è di overfitting. Aumentare i campioni di training potrebbe aiutare, ma non è detto che lo faccia.
	\item[Ridurre il set di Features.] Si potrebbe pensare di ridurre il numero di features in input al modello, in modo da semplificare il problema e ridurre il rischio di overfitting. Questa tecnica può essere efficace se alcune features sono ridondanti o non rilevanti per il task. Tuttavia, è importante fare attenzione a non eliminare informazioni utili. 
	\item[Cercare nuove features.] L'opposto della riduzione delle features: il problema è di underfitting, in quanto il modello non ha abbastanza informazioni per generalizzare bene. Aggiungere nuove features può aiutare a migliorare le prestazioni del modello ma richiede un'attenta selezione delle stesse per evitare di introdurre rumore.
	\item[Utilizzare features polinomiali.] Trasformare le features esistenti in polinomiali può aiutare a catturare relazioni non lineari tra le variabili. Questa tecnica può essere utile se si sospetta che il modello lineare non sia sufficiente per rappresentare i dati. Tuttavia, l'aggiunta di termini polinomiali aumenta la complessità del modello e il rischio di overfitting, quindi è importante bilanciare questa scelta con tecniche di regolarizzazione.
	\item[Far variare il parametro di regolarizzazione $\lambda$.] Modificare il parametro di regolarizzazione può influenzare significativamente le prestazioni del modello. Un valore più alto di $\lambda$ penalizza maggiormente i pesi del modello, riducendo il rischio di overfitting, mentre un valore più basso consente al modello di adattarsi meglio ai dati di training, ma aumenta il rischio di overfitting. È importante trovare un equilibrio ottimale attraverso tecniche come la validazione incrociata (che vedremo più avanti). 
\end{description}

Solitamente si cerca di effettuare test di diagnostica applicati al modello, per permettere di ottenere informazioni dettagliate su cosa funziona o non funziona con l'obiettivo di ottenere indicazioni per migliorare le prestazioni.

\section{Valutazione}
La valutazione di un algoritmo di ML è fondamentale per capire se il modello è adatto al task. 

\subsection{Valutazione incrociata}
Un modo di fare valutazione incrociata\footnote{Per valutazione incrociata si intende la tecnica di suddividere il dataset in più parti per allenare e testare il modello in modo iterativo, garantendo una stima più robusta delle prestazioni.} è la \textbf{grid search}, che sfrutta la k-Fold Cross Validation: si suddivide il dataset in $t$ splits e in $k$ folds, e si allena il modello su $t-1$ splits e si valuta su uno split di validation, ripetendo il processo per tutti i $t$ splits. In questo modo si ottengono $t$ stime delle prestazioni del modello, che vengono poi mediate per ottenere una stima complessiva. Questo processo viene ripetuto per tutti i possibili set di iperparametri del modello, selezionando quelli che producono la \emph{migliore prestazione} media sul validation set. Infine, si valuta il modello finale con gli iperparametri selezionati sul test set per ottenere una stima finale delle prestazioni del modello.

\paragraph{Procedura.}
La procedura è la seguente:
\begin{enumerate}
	\item Si suddivide il dataset originale in training/validation set e un test set che non verrà usato fino alla fine della valutazione.
	\item Si suddivide il dataset rimanente in $t$ splits e in $k$ folds.
	\item Si seleziona uno split come validation set e gli altri $t-1$ come training set.
	\item Si allena il modello sui $t-1$ training splits.
	\item Si valuta il modello con una funzione $J_{\text{VAL}}$ sul validation fold (non usato per il training).
	\item Si ripete il processo per tutti i $t$ splits, ottenendo $t$ valori di $J_{\text{VAL}}$.
	\item Si calcola la media dei $t$ valori di $J_{\text{VAL}}$ per ottenere una stima complessiva delle prestazioni del modello.
	\item Si ripete l'intero processo per tutti i possibili set di iperparametri del modello.
	\item Si selezionano gli iperparametri che hanno prodotto la migliore prestazione media sul validation set.
	\item Infine, si valuta il modello finale con gli iperparametri selezionati sul test set per ottenere una stima finale delle prestazioni del modello.
\end{enumerate}

In questo modo il modello è robusto perché è stato allenato su diversi training set per i dati, allenato sui validation set per gli iperparametri e testato su un test set \textbf{mai visto prima}.

\paragraph{Esempio.}
Ipotizziamo di avere un certo dataset, dividiamolo in un test set (20\% dei dati) e in un training/validation set (80\% dei dati). Definiamo le funzioni di costo:
\begin{itemize}
	\item \textbf{Funzione di costo di training:}
	\[
	J_{\text{TR}}(\vartheta) = \frac{1}{2m_{\text{TR}}} \sum_{i=1}^{m_{\text{TR}}} \left( h_\vartheta(x^{(i)}_{\text{TR}}) - y^{(i)}_{\text{TR}} \right)^2
	\]
	Dove $m_{\text{TR}}$ è il numero di campioni nel training set, $x^{(i)}_{\text{TR}}$ e $y^{(i)}_{\text{TR}}$ sono rispettivamente l'input e l'output del $i$-esimo campione del training set.
	\item \textbf{Funzione di costo di validation:}
	\[
	J_{\text{VAL}}(\vartheta) = \frac{1}{2m_{\text{VAL}}} \sum_{i=1}^{m_{\text{VAL}}} \left( h_\vartheta(x^{(i)}_{\text{VAL}}) - y^{(i)}_{\text{VAL}} \right)^2
	\]
	Dove $m_{\text{VAL}}$ è il numero di campioni nel validation set, $x^{(i)}_{\text{VAL}}$ e $y^{(i)}_{\text{VAL}}$ sono rispettivamente l'input e l'output del $i$-esimo campione del validation set.
	\item \textbf{Funzione di costo di test:}
	\[
	J_{\text{TEST}}(\vartheta) = \frac{1}{2m_{\text{TEST}}} \sum_{i=1}^{m_{\text{TEST}}} \left( h_\vartheta(x^{(i)}_{\text{TEST}}) - y^{(i)}_{\text{TEST}} \right)^2
	\]
	Dove $m_{\text{TEST}}$ è il numero di campioni nel test set, $x^{(i)}_{\text{TEST}}$ e $y^{(i)}_{\text{TEST}}$ sono rispettivamente l'input e l'output del $i$-esimo campione del test set.
\end{itemize}

\noindent
Supponiamo di voler utilizzare $t=5$ splits e $k=5$ folds. La suddivisione del training/validation set sarà la seguente:

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{images/splits_folds_example.png}
	\caption{Dataset diviso in split e folds per la valutazione incrociata.}
\end{figure}

Dopo aver suddiviso i dati, ognuno dei dati produrrà una stima di $J_{\text{VAL}}^H(\vartheta^{(i)})$ per un certo $i$-esimo set valutata sull'iperparametro $H$. Da queste andiamo a fare la media:
\[
\bar{J}_{\text{VAL}}^H(\vartheta) = \frac{1}{t} \sum_{i=1}^{t} J_{\text{VAL}}^H(\vartheta^{(i)})
\]

Ottenuta la media $\bar{J}_{\text{VAL}}^H(\vartheta)$ per ogni set di iperparametri $H$, si seleziona quello che minimizza la funzione di costo media:
\[
H^* = \arg\min_H \bar{J}_{\text{VAL}}^H(\vartheta)
\]

Una volta fissato il miglior iperparametro $H^*$, si allena il modello sul test set per ottenere la stima finale:
\[
J_{\text{TEST}}^{H^*}(\vartheta)
\]

\subsection{Bias e Varianza}
Per valutare le prestazioni di un modello possiamo usare i valori del bias e della varianza per fare delle stime su come il modello si comporta sui dati.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/ovfit_underfit_rightfit.png}
	\caption{Underfitting---capacità adeguata---overfitting in regressione polinomiale: punti blu \(=\) dati di training (\(\mathrm{TR}\)), punti rossi \(=\) fuori da \(\mathrm{TR}\); la linea verde mostra il fit del modello: lineare (sinistra), polinomio di grado \(2\) (centro) e polinomio di grado \(5\) (destra).}
	\label{fig:ovfit_underfit_rightfit}
\end{figure}

\paragraph{Bias.} Ricordiamo che il bias è \textbf{l'errore sistematico} che il modello commette sui dati. Un modello con alto bias tende a sottostimare la complessità del problema, portando a errori elevati sia sul training set che sul test set. Questo fenomeno è noto come \textbf{underfitting}. Nell'immagine \ref{fig:ovfit_underfit_rightfit}, il grafico a sinistra mostra un esempio di underfitting, dove il modello lineare non riesce a catturare la relazione tra le variabili e commette un errore sistematico sia sul training set (punti blu) che sul test set (punti rossi).

\paragraph{Varianza.} La varianza rappresenta la sensibilità del modello alle variazioni nei dati di training. Un modello con alta varianza tende a sovradattarsi ai dati di training, catturando il rumore invece della vera relazione tra le variabili. Questo porta a errori bassi sul training set ma elevati sul test set, fenomeno noto come \textbf{overfitting}. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.75\textwidth]{images/bias_variance_tradeoff.png}
	\caption{Trade-off tra bias e varianza in funzione della complessità del modello.}
	\label{fig:bias_variance_tradeoff}
\end{figure}
 
Nella figura \ref{fig:ovfit_underfit_rightfit}, il grafico a destra mostra un esempio di overfitting, dove il modello polinomiale è troppo complesso e si adatta troppo strettamente ai dati di training, risultando in prestazioni scadenti sui dati di test\footnote{Si noti che basterebbe rimuovere un punto del training set per far cambiare completamente il modello, in quanto altamente sensibile ai dati di input.}.
Come si vede dall'immagine al centro della figura \ref{fig:ovfit_underfit_rightfit}, un modello con capacità adeguata riesce a bilanciare bias e varianza, ottenendo buone prestazioni sia sul training set che sul test set. Per valutare e risolvere i problemi sul modello, possiamo andare a controllare come Bias e Varianza variano al variare dei parametri del modello\footnote{In quella che molti chiamano \textbf{bias-variance trade-off}.} e notiamo che si può costruire una correlazione tra l'errore commesso dal modello (quindi o in $J_{\text{TEST}}$ o in $J_{\text{VAL}}$) e la complessità del polinomio (il grado):


\paragraph{Parametro di regolarizzazione.}

Anche i parametri di regolarizzazione, esattamente come il modello (i suoi parametri), devono essere stabiliti sperimentalmente. Si parte da $\lambda^0 = 0$, testando valori di $\lambda^{(1)}, \lambda^{(2)} \ldots \lambda^{(k)}$ sempre più grandi, verificando come cambia $J_{\text{VAL}}(\vartheta^i)$ in funzione di $\lambda^i$:
\[
\left[
\begin{array}{c}
\lambda^{(0)}\\
\lambda^{(1)}\\
\lambda^{(2)}\\
\lambda^{(3)}\\
\vdots\\
\lambda^{(k)}
\end{array}
\right]
\;\Rightarrow\;
\min_{\vartheta} J(\vartheta)
\;\Rightarrow\;
\left[
\begin{array}{c}
\vartheta^{(0)}\\
\vartheta^{(1)}\\
\vdots \\
\vartheta^{(k)}
\end{array}
\right]
\quad
\left[
\begin{array}{c}
J_{\text{VAL}}(\vartheta^{(0)})\\
J_{\text{VAL}}(\vartheta^{(1)})\\
\vdots \\
J_{\text{VAL}}(\vartheta^{(k)})
\end{array}
\right]
\;\Rightarrow\;
\underbrace{\text{BEST } J_{\text{VAL}}}_{\text{min}}
\;\Rightarrow\;
J_{\text{TEST}}\!\big(\vartheta^{\text{BEST}}\big)
\]


In questo modo si riesce a trovare il miglior parametro di regolarizzazione che minimizza l'errore sul validation set, e si può usare questo parametro per valutare il modello sul test set.

\paragraph{Altre curve.}
Un'altra curva potrebbe essere quella che mostra l'\textbf{errore} su \emph{train set} e \emph{validation set} in funzione del numero di campioni di training (figura \ref{fig:error_vs_training_size}). In questo modo si può capire se il modello soffre di overfitting o underfitting:

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{images/learning_curve_train_validation.png}
	\caption{Learning curves: errore su training \(J_{\mathrm{TRAIN}}\) e su validation \(J_{\mathrm{VAL}}\) in funzione del numero di esempi \(m\). \(J_{\mathrm{VAL}}\) decresce, \(J_{\mathrm{TRAIN}}\) cresce leggermente e il \emph{generalization gap} (freccia) si riduce; le curve si avvicinano all'errore irreducibile.}
	\label{fig:error_vs_training_size}
\end{figure}

La figura conferma il fatto che esiste il trade-off bias-varianza, in quanto:
\begin{itemize}
	\item Se siamo in presenza di \emph{high bias}, quindi un alto gap tra la curva di train e l'asse delle ascisse, aumentare il numero di dati di training non aiuta molto. 
	\item Se siamo in presenza di \emph{high variance}, quindi un grande gap tra l'errore commesso tra il training set e il validation set, aumentare il numero di dati di training potrebbe aiutare a migliorare i risultati (non sempre).
\end{itemize}

