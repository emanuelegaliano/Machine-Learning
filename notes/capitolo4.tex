\chapter{Classificazione}

\section{Introduzione alla classificazione}
La \textbf{classificazione} è un altro dei problemi tipici che può essere risolto tramite un algoritmo di Machine Learning. Riguarda lo \textbf{stabilire se un dato input appartiene ad una tra delle classi} prestabilite dal problema. Il\textbf{ target non sarà più un valore} $\in \mathbb{R}$, ma una tra $k$ classi.
Per studiare la classificazione, andremo a stabilire i requisiti e i nostri obiettivi.
\begin{itemize}
	\item Un modello ben definito.
	\item Una funzione di loss per stabilire lo scarto con i risultati attesi.
	\item Un algoritmo di learning per trovare i parametri.
	\item Delle misure di valutazione.
	\item Niente overfitting!
\end{itemize}

\subsection{Classificazione: binaria e di classe}
Identifichiamo due tipi di task di classificazione:

\begin{itemize}
	\item \textbf{Classificazione binaria.}\\
	Le etichette $y \in \{ 0,1\}$. Va associata ad $x$ una delle due etichette, o le probabilità relativa a ciascuna delle due.
	\item \textbf{Classificazione di classe.}\\
	Le etichette $y \in \{ 0,1, \dots, k-1\}$. Va associata ad $x$ una delle etichette, o la probabilità relativa a ciascuna di esse.
\end{itemize}

È fondamentale sottolineare l'importanza che gli output di questi algoritmi siano sempre $\in[0,1]$, in quanto da interpretare come \textbf{probabilità}.
\section{Classificazione binaria}
Nonostante i classificatori binari possano sembrare limitati, questi trovano applicazioni in vari task: solitamente, questi si basano sulla suddivisione di immagini più grandi in \textit{patches} più piccole, con una successiva, appunto, classificazione, per individuare determinati elementi. Nel \textbf{medical imaging}, i classificatori binari possono individuare piccoli tumori. Nei \textbf{controlli qualità }in ambito industriale, possono individuare \textbf{imperfezioni} nei materiali. Nel campo più generale della \textbf{computer vision}, gli algoritmi di classificazione (binaria e non) sono fondamentali.

\subsection{Perché non usare la regressione lineare per la classificazione?}

Si potrebbe pensare di \textbf{usare un modello di regressione lineare per effettuare classificazione binaria}, e l'intuizione non sarebbe nemmeno totalmente sbagliata: potremmo usare, ad esempio, la regressione per \textbf{trovare una retta che unisce i dati }in maniera opportuna, e in funzione della pendenza di questa retta, \textbf{stabilire un valore di sogliatura }per distinguere due classi.

Forniamo il seguente esempio: vogliamo addestrare un modello per prevedere se un tumore è benigno o maligno usando una singola feature (dimensione del tumore).

Usiamo la regressione lineare e facciamo il fitting di una retta nello spazio (troviamo la retta, cioè la funzione che meglio si posiziona a media tra tutti i punti). Possiamo trovare il miglior $y = \vartheta^Tx$ dal nostro set di dati e quindi selezionare una \textbf{soglia} su $y$ per classificare quando il tumore è maligno o meno:
$h_\vartheta(x) \leq 0.5 \to 0, \quad h_\vartheta(x) \geq 0.5 \to 1$.

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.7\linewidth]{images/regression-for-classification.png}
\end{figure}



Ci sono \textbf{due criticità} relative all'utilizzo della regressione. La prima, riguarda la versatilità del modello: il fitting della retta non è opportuno, in  quanto la regressione lineare dipende fortemente dalla distribuzione dei dati.


\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.7\linewidth]{images/regression-for-classification2}
\end{figure}
\newpage
La seconda criticità riguarda invece la $y$, non sempre limitata tra 0 e 1, come ci aspettiamo in un modello di classificazione binaria. Come ci comportiamo con valori più grandi di 1? E con valori minori di 0? 

\section{Regressione logicistica - Sigmoide}
Se la regressione lineare restituisce una $y$ non limitata tra 0 e 1, la \textbf{regressione logistica} risolve il problema con una funzione, detta \textbf{sigmoide}, applicata sul risultato $z = \vartheta^TX$ del modello di regressione. La \textbf{sigmoide} una funzione del tipo

$$
\overline{\sigma}(z) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-\vartheta^TX}}
$$
\noindent
con $z = \vartheta_0 + \vartheta_1 x_1 + \dots + \vartheta_i x_i$. Questa funzione ha un'interpretazione probabilistica molto banale, opportuna per \textbf{adattare i modelli di regressione lineare su problemi di classificazione}\footnote{Non si può pretendere di usare un modello di regressione lineare su task di classificazione, ma è comunque possibile, tramite l'uso della sigmoide, sfruttarlo per task di classificazione binaria.}. Osserviamo che:

\begin{itemize}
	\item $1-\overline{\sigma} = \sigma(z)$
	\item $\displaystyle\frac{\partial \overline{\sigma}(z)}{\partial t} =\overline{\sigma}(z)(1-\overline{\sigma}(z)) = \overline{\sigma}(z)\overline{\sigma}(-z)$
	\item È inoltre dimostrabile che $\displaystyle\overline{\sigma} = \frac{1}{2} + \frac{1}{2}\tanh\left(\frac{z}{2}\right)$. Questo ci apre le porte a implementazioni molto più semplici.
\end{itemize}

Concludiamo che la regressione logicistica, è ottimale per trovare opportuni parametri di $\vartheta^T$, che moltiplicati a $x$, ci permetteranno di stabilire il \textbf{decision boundary}, nella ben nota forma:

$$
\vartheta_0x_0 + \vartheta_1x_1 + \vartheta_2x_2 + \dots \vartheta_dx_d  
$$

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.75\linewidth]{images/sigmoid}
\end{figure}


\subsection{Vantaggi relativi all'uso della sigmoide}

\begin{itemize}
	\item Interpretazione probabilistica semplice. Da valori $\in[0,1]$.
	\item Derivabile, più liscia rispetto ad una step function.
	\item Introduce non lineareità, migliorando la classificazione.
\end{itemize}

\section{Funzione di Loss}
Avere una funzione di Loss associata al modello è fondamentale per capire come effettuare il training, e quali sono i migliori parametri di $\vartheta^T$ per la classificazione. 
Per modellare questa funzione (da minimizzare secondo l'algoritmo di discesa del gradiente), utilizzeremo due funzioni logaritmiche. La funzione di Loss sarà definita in funzione dell'etichetta $\hat{y}$.
$$
\text{Loss}(h_\vartheta(x), y) = \begin{cases}
	- \log (h_\vartheta(x)) \qquad\qquad\text{se } \hat{y} = 1\\
	- \log (1-h_\vartheta(x)) \qquad\text{ se } \hat{y} = 0\\
\end{cases}
$$

Questo, perché l'errore deve essere 1 quando $\hat{y} = 0$ e $y = 1$, o quando $\hat{y} = 1$ e $y = 0$.

\begin{center}
	\textit{Ai fini della semplicità e univocità della spiegazione, useremo $y$ per indicare ciò che fino ad ora abbiamo indicato con $\hat{y}$.}
\end{center}


\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.50\linewidth]{images/loss-logistic-regression}
\end{figure}
\noindent
Possiamo esprimere la funzione di Loss anche in questo modo, rendendo unica la definizione della funzione.
$$
\text{Loss}(h_\vartheta(x), y) = -[y\log (h_\vartheta(x)) + (1-y)\log (1 - h_\vartheta(x))]
$$
\noindent
Definita la funzione di Loss, andremo a definire la funzione di costo

$$
J(\vartheta) = - \frac{1}{m} \sum_{i=1}^{m}(y^{(i)}\log (h_\vartheta(x)) + (1-y^{(i)})\log (1 - h_\vartheta(x)))
$$

\noindent
su cui sarà possibile applicare l'algoritmo di discesa del gradiente, fino alla convergenza del modello.
$$
\vartheta_j^{\text{new}} = \vartheta_j^{\text{old}} -\alpha\frac{\partial J(\vartheta)}{\partial \vartheta_j} \qquad \forall j
$$

\subsection{Derivata parziale della funzione di costo}

Necessaria per applicare la discesa del gradiente.
$$
\frac{\partial J(\vartheta)}{\partial \vartheta_j} = \frac{1}{m} \sum_{i=1}^{m} (\underbrace{h_\vartheta(x^{(i)})}_{(*)}- y^{(i)}))x_j^{(i)}) \qquad \forall j = 0, \dots, d
$$
(*) Questo membro nasconde la funzione sigmoide $\overline{\sigma}(\vartheta^TX^{(i)})$.

\subsection{Entropia dell'informazione}

La loss function di questo modello presenta un'interpretazione probabilistica basata sul concetto di \textbf{entropia} dell'informazione. Parleremo quindi \textbf{Binary Cross Entropy Loss}. La seguente, è la formula dell'entropia:

$$
H(X) = -\sum_{k=1}^{K} p_k\log_2(p_k)
$$

con $p_k$ probabilità di essere di classe $k$.  La distribuzione di probabilità (e quindi l'entropia). Queste distribuzioni informazioni su:
\begin{itemize}
	\item Incertezza.
	\item Misura del disordine dalle classi.
	\item Quanto la probabilità è concentrata su una classe.
	\item Informazione (la distribuzione è molto informativa se è capace di distinguere in maniera opportuna).
\end{itemize}
Un esempio su $k=2$ è la funzione 
$$
H(X) = - p_1\log_2(p_1) - p_2\log_2(p_2) = -[p_1\log_2(p_1) + p_2\log_2(p_2) ]
$$
\begin{figure}[tbph]
	\centering
	\includegraphics[width=1\linewidth]{images/cross_entropy}
\end{figure}

\textit{$\log_2(0)$ è indefinito. Quando facciamo calcoli relativi alla cross-entropy, consideriamo valori piccoli, mai nulli. In questo esempio e nel prossimo esercizio, quando utilizziamo 0, stiamo approssimando un valore molto piccolo, ma mai effettivamente nullo. Inoltre, in un modello reale, avere una $p_k = 0$ o $p_k = 1$ significa probabile overfitting del modello.}
\newpage

\subsubsection{Esercizio sull'entropia}
Riordina le seguenti distribuzioni in ordine crescente di entropia.

\begin{figure}[tbph]
	\centering
	\includegraphics[width=1\linewidth]{entropia-esercizio}
\end{figure}

$$
H(X) = - p_1\log_2(p_1) - p_2\log_2(p_2) = -[p_1\log_2(p_1) + p_2\log_2(p_2) + p_3\log_2(p_3) ]
$$

\begin{enumerate}
	\item $H(X_1) =  -[0\log(0) + 1\log(1) + 0\log(0) ] = 0$
	\item $H(X_2) =  -[1\log(1) + 0\log(0) + 0\log(0) ] = 0$
	\item $H(X_3) =  -[0,33\log(0,33) + 0,33\log(0,33) + 0,33\log(0,33) ] \approx 1,5848$
	\item $H(X_4) =  -[0,80\log(0,80) + 0,20\log(0,20) + 0\log(0) ] \approx 0,722$
\end{enumerate}

$$
X_1 = X_2 < X_4 < X_3
$$

\subsection{Binary Cross Entropy Loss}

La seguente formula è equivalente alla nostra funzione di Loss, ma fornisce un'interpretazione più intuitiva. Definiamo la \textbf{Binary Cross Entropy Loss} come:

$$
H_{BCE}(X) = \frac{1}{m}\sum_{i=1}^{m} \left(-\sum_{k=1}^{K}\underbrace{p_k^{(i)}}_{\text{(a)}} \log_2(\underbrace{q_k^{(i)}}_{\text{(b)}})\right)
$$

Dove (a) è la distribuzione vera della classificazione, detta \textbf{ground truth}, ovvero quella con probabilità massima sulla classe attesa, mentre (b) è la distribuzione data dal classificatore. Misura quindi la \textbf{distanza tra le due distribuzioni}. Minimizzare $H_{BCE}(X)$, significherà rendere le distribuzioni ideali e quelle effettive del classificatore, quanto più simili possibile.

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.9\linewidth]{images/coss-entropy2}
\end{figure}


\newpage

\section{Overfitting nella classificazione}
Rischiamo di cadere in overfitting nell'utilizzo di classificatori polinomiali. Basterà usare i termini di regolarizzazione per diminuire o annullare l'impatto di alcuni termini di grado eccessivamente alto, ottenendo così:
$$
J(\vartheta) = - \frac{1}{m} \sum_{i=1}^{m}(y^{(i)}\log (h_\vartheta(x)) + (1-y^{(i)})\log (1 - h_\vartheta(x))) + \frac{\lambda}{2m}\sum_{j=1}^n\vartheta_j^2
$$ 

\begin{figure}[tbph]
	\centering
	\includegraphics[width=1\linewidth]{images/underoverfitting-classification}
\end{figure}

\section{Reject Region, regione d'incertezza}

Sia nella classificazione binaria, che in quella multiclasse, è possibile osservare casi in cui la probabilità stimata che un input appartenga ad una tra le classi specificate, non superi un determinato valore di certezza. Sono dei casi limite che vanno gestiti in maniera opportuna:
\begin{enumerate}
	\item Prendere il valore con certezza più alta (se è proprio obbligatorio stabilire una classe, e il task non è particolarmente delicato).
	\item Scartare l'input.
\end{enumerate}

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.47\linewidth]{images/rejection-region}
	\caption{La zona d'incertezza (in cui $P(C_1|x) \sim P(C_2|x)$)}
\end{figure}


\newpage

\section{Classificazione multiclasse e one vs all}  
La classificazione multiclasse si distingue da quella binaria per il numero di classi.
Il dataset di input non avrà più etichette binarie, ma $k$ etichette. A ogni classe si associa un numero, ai fini di semplicità. 

\subsection{Metodo One vs All}
Possiamo ottenere una classificazione multiclasse da dei classificatori binari, usando la metodologia \textbf{one vs all}. Immaginiamo di avere un sistema di classificazione figure geometriche, con le classi \textit{triangolo}, \textit{quadrato }e \textit{cerchio }($c_1,c_2,c_3$).
Avremo bisogno di tre classificatori, $h_{\vartheta}^1, h_{\vartheta}^2, h_{\vartheta}^3$, capaci di misurare $P(\text{triangolo}|x), P(\text{quadrato}|x), $ $P(\text{cerchio}|x)$. Prenderemo poi l'etichetta associata al valore di probabilità più alto

$$
\hat{k} = \text{arg}\max_k h_{\vartheta}^k(\overline{x})
$$

ottenendo effettivamente una classificazione multiclasse. 

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.8\linewidth]{images/onevsrest}
\end{figure}

Il training è effettuato sui singoli classificatori binari.
Osserviamo inoltre che, con $k$ classi:

$$
\text{Numero di classificatori richiesti in one vs all} = k 
$$

\subsection{Metodo One vs One}
In questo caso, i classificatori distinguono classi a due a due. Si fanno poi le opportune valutazioni per capire quale classe assegnare. Il numero di classificatori risulta essere più alto. Con $k$ classi abbiamo:

$$
\text{Numero di classificatori richiesti in one vs one} = \frac{k(k-1)}{2}
$$

Immaginiamo un classificatore per lo stesso problema precedentemente esposto: otteniuamo le classi $(c_1,c_2,c_3)$. Questi classificatori su coppie, daranno poi degli opportuni risultati:
$$
c_1 \text{ vs } c_2, \quad c_2 \text{ vs } c_3, \quad c_1 \text{ vs } c_3
$$

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.75\linewidth]{images/onevsone}
\end{figure}

La classe con più voti è quella predetta. 

\subsection{Confronto tra \textit{One vs All} e \textit{One vs One}}
Il modello one vs one, per quanto più oneroso in termini di numero di classificatori binari richiesti, non presenta l'area di incertezza su tutte le classi, che invece possiamo trovare nel classificatore one vs all.

\begin{figure}[tbph]
	\centering
	\includegraphics[width=1\linewidth]{images/onevsonevsonevsall}
\end{figure}

