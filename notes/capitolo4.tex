\chapter{Classificazione}

\section{Introduzione alla classificazione}
La \textbf{classificazione} è un altro dei problemi tipici che può essere risolto tramite un algoritmo di Machine Learning. Riguarda lo \textbf{stabilire se un dato input appartiene ad una tra delle classi} prestabilite dal problema. Il\textbf{ target non sarà più un valore} $\in \mathbb{R}$, ma una tra $k$ classi.
Per studiare la classificazione, andremo a stabilire i requisiti e i nostri obiettivi.
\begin{itemize}
	\item Un modello ben definito.
	\item Una funzione di loss per stabilire lo scarto con i risultati attesi.
	\item Un algoritmo di learning per trovare i parametri.
	\item Delle misure di valutazione.
	\item Niente overfitting!
\end{itemize}

\subsection{Classificazione: binaria e di classe}
Identifichiamo due tipi di task di classificazione:

\begin{itemize}
	\item \textbf{Classificazione binaria.}\\
	Le etichette $y \in \{ 0,1\}$. Va associata ad $x$ una delle due etichette, o le probabilità relativa a ciascuna delle due.
	\item \textbf{Classificazione di classe.}\\
	Le etichette $y \in \{ 0,1, \dots, k-1\}$. Va associata ad $x$ una delle etichette, o la probabilità relativa a ciascuna di esse.
\end{itemize}

È fondamentale sottolineare l'importanza che gli output di questi algoritmi siano sempre $\in[0,1]$, in quanto da interpretare come \textbf{probabilità}.
\section{Classificazione binaria}
Nonostante i classificatori binari possano sembrare limitati, questi trovano applicazioni in vari task: solitamente, questi si basano sulla suddivisione di immagini più grandi in \textit{patches} più piccole, con una successiva, appunto, classificazione, per individuare determinati elementi. Nel \textbf{medical imaging}, i classificatori binari possono individuare piccoli tumori. Nei \textbf{controlli qualità }in ambito industriale, possono individuare \textbf{imperfezioni} nei materiali. Nel campo più generale della \textbf{computer vision}, gli algoritmi di classificazione (binaria e non) sono fondamentali.

\subsection{Perché non usare la regressione lineare per la classificazione?}

Si potrebbe pensare di \textbf{usare un modello di regressione lineare per effettuare classificazione binaria}, e l'intuizione non sarebbe nemmeno totalmente sbagliata: potremmo usare, ad esempio, la regressione per \textbf{trovare una retta che unisce i dati }in maniera opportuna, e in funzione della pendenza di questa retta, \textbf{stabilire un valore di sogliatura }per distinguere due classi.

Forniamo il seguente esempio: vogliamo addestrare un modello per prevedere se un tumore è benigno o maligno usando una singola feature (dimensione del tumore).

Usiamo la regressione lineare e facciamo il fitting di una retta nello spazio (troviamo la retta, cioè la funzione che meglio si posiziona a media tra tutti i punti). Possiamo trovare il miglior $y = \vartheta^Tx$ dal nostro set di dati e quindi selezionare una \textbf{soglia} su $y$ per classificare quando il tumore è maligno o meno:
$h_\vartheta(x) \leq 0.5 \to 0, \quad h_\vartheta(x) \geq 0.5 \to 1$.

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.7\linewidth]{images/regression-for-classification.png}
\end{figure}



Ci sono \textbf{due criticità} relative all'utilizzo della regressione. La prima, riguarda la versatilità del modello: il fitting della retta non è opportuno, in  quanto la regressione lineare dipende fortemente dalla distribuzione dei dati.


\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.7\linewidth]{images/regression-for-classification2}
\end{figure}
\newpage
La seconda criticità riguarda invece la $y$, non sempre limitata tra 0 e 1, come ci aspettiamo in un modello di classificazione binaria. Come ci comportiamo con valori più grandi di 1? E con valori minori di 0? Possiamo risolvere il problema introducendo delle funzioni, dette funzioni \textbf{di linking}. Queste ci permettono di \textbf{adattare i nostri valori }a modelli di regressione che altrimenti non sarebbero opportuni per la classificazione. Vediamo un esempio relativo alla regressione logistica usando la funzione \textbf{sigmoide}.

\section{Regressione logicistica - Sigmoide}
Se la regressione lineare restituisce una $y$ non limitata tra 0 e 1, la \textbf{regressione logistica} risolve il problema con una funzione, detta \textbf{sigmoide}, applicata sul risultato $z = \vartheta^TX$ del modello di regressione. La \textbf{sigmoide} una funzione del tipo

$$
\overline{\sigma}(z) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-\vartheta^TX}}
$$
\noindent
con $z = \vartheta_0 + \vartheta_1 x_1 + \dots + \vartheta_i x_i$. Questa funzione ha un'interpretazione probabilistica molto banale, opportuna per \textbf{adattare i modelli di regressione lineare su problemi di classificazione}\footnote{Non si può pretendere di usare un modello di regressione lineare su task di classificazione, senza fare alcun tipo di modifica, come quelle che introduciamo con la sigmoide.}. Osserviamo che:

\begin{itemize}
	\item $1-\overline{\sigma} = \sigma(z)$
	\item $\displaystyle\frac{\partial \overline{\sigma}(z)}{\partial t} =\overline{\sigma}(z)(1-\overline{\sigma}(z)) = \overline{\sigma}(z)\overline{\sigma}(-z)$
	\item È inoltre dimostrabile che $\displaystyle\overline{\sigma} = \frac{1}{2} + \frac{1}{2}\tanh\left(\frac{z}{2}\right)$. Questo ci apre le porte a implementazioni molto più semplici.
\end{itemize}

Concludiamo che la regressione logicistica, è ottimale per trovare opportuni parametri di $\vartheta^T$, che moltiplicati a $x$, ci permetteranno di stabilire il \textbf{decision boundary}, nella ben nota forma:

$$
\vartheta_0x_0 + \vartheta_1x_1 + \vartheta_2x_2 + \dots \vartheta_dx_d  
$$

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.6\linewidth]{images/sigmoid}
\end{figure}


\subsection{Vantaggi relativi all'uso della sigmoide}

\begin{itemize}
	\item Interpretazione probabilistica semplice. Da valori $\in[0,1]$.
	\item Derivabile, più liscia rispetto ad una step function.
	\item Introduce non lineareità, migliorando la classificazione.
\end{itemize}

\section{Funzione di Loss}
Avere una funzione di Loss associata al modello è fondamentale per capire come effettuare il training, e quali sono i migliori parametri di $\vartheta^T$ per la classificazione. 
Per modellare questa funzione (da minimizzare secondo l'algoritmo di discesa del gradiente), utilizzeremo due funzioni logaritmiche. La funzione di Loss sarà definita in funzione dell'etichetta $\hat{y}$.
$$
\text{Loss}(h_\vartheta(x), y) = \begin{cases}
	- \log (h_\vartheta(x)) \qquad\qquad\text{se } \hat{y} = 1\\
	- \log (1-h_\vartheta(x)) \qquad\text{ se } \hat{y} = 0\\
\end{cases}
$$

Questo, perché l'errore deve essere 1 quando $\hat{y} = 0$ e $y = 1$, o quando $\hat{y} = 1$ e $y = 0$.

\begin{center}
	\textit{Ai fini della semplicità e univocità della spiegazione, useremo $y$ per indicare ciò che fino ad ora abbiamo indicato con $\hat{y}$.}
\end{center}


\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.50\linewidth]{images/loss-logistic-regression}
\end{figure}
\noindent
Possiamo esprimere la funzione di Loss anche in questo modo, rendendo unica la definizione della funzione.
$$
\text{Loss}(h_\vartheta(x), y) = -[y\log (h_\vartheta(x)) + (1-y)\log (1 - h_\vartheta(x))]
$$
\noindent
Definita la funzione di Loss, andremo a definire la funzione di costo

$$
J(\vartheta) = - \frac{1}{m} \sum_{i=1}^{m}(y^{(i)}\log (h_\vartheta(x)) + (1-y^{(i)})\log (1 - h_\vartheta(x)))
$$

\noindent
su cui sarà possibile applicare l'algoritmo di discesa del gradiente, fino alla convergenza del modello.
$$
\vartheta_j^{\text{new}} = \vartheta_j^{\text{old}} -\alpha\frac{\partial J(\vartheta)}{\partial \vartheta_j} \qquad \forall j
$$

\subsection{Derivata parziale della funzione di costo}

Necessaria per applicare la discesa del gradiente.
$$
\frac{\partial J(\vartheta)}{\partial \vartheta_j} = \frac{1}{m} \sum_{i=1}^{m} (\underbrace{h_\vartheta(x^{(i)})}_{(*)}- y^{(i)}))x_j^{(i)}) \qquad \forall j = 0, \dots, d
$$
(*) Questo membro nasconde la funzione sigmoide $\overline{\sigma}(\vartheta^TX^{(i)})$.

\subsection{Entropia dell'informazione}

La loss function di questo modello presenta un'interpretazione probabilistica basata sul concetto di \textbf{entropia} dell'informazione. Parleremo quindi \textbf{Binary Cross Entropy Loss}. La seguente, è la formula dell'entropia:

$$
H(X) = -\sum_{k=1}^{K} p_k\log_2(p_k)
$$

con $p_k$ probabilità di essere di classe $k$.  La distribuzione di probabilità (e quindi l'entropia). Queste distribuzioni informazioni su:
\begin{itemize}
	\item Incertezza.
	\item Misura del disordine dalle classi.
	\item Quanto la probabilità è concentrata su una classe.
	\item Informazione (la distribuzione è molto informativa se è capace di distinguere in maniera opportuna).
\end{itemize}
Un esempio su $k=2$ è la funzione 
$$
H(X) = - p_1\log_2(p_1) - p_2\log_2(p_2) = -[p_1\log_2(p_1) + p_2\log_2(p_2) ]
$$

\begin{figure}[tbph]
	\centering
	\includegraphics[width=1\linewidth]{images/entropy_sketch}
\end{figure}


\textit{$\log_2(0)$ è indefinito. Quando facciamo calcoli relativi alla cross-entropy, consideriamo valori piccoli, mai nulli. In questo esempio e nel prossimo esercizio, quando utilizziamo 0, stiamo approssimando un valore molto piccolo, ma mai effettivamente nullo. Inoltre, in un modello reale, avere una $p_k = 0$ o $p_k = 1$ significa probabile overfitting del modello.}
\newpage

\subsubsection{Esercizio sull'entropia}
Riordina le seguenti distribuzioni in ordine crescente di entropia.

\begin{figure}[tbph]
	\centering
	\includegraphics[width=1\linewidth]{./images/entropia-esercizio}
\end{figure}

$$
H(X) = - p_1\log_2(p_1) - p_2\log_2(p_2) = -[p_1\log_2(p_1) + p_2\log_2(p_2) + p_3\log_2(p_3) ]
$$

\begin{enumerate}
	\item $H(X_1) =  -[0\log(0) + 1\log(1) + 0\log(0) ] = 0$
	\item $H(X_2) =  -[1\log(1) + 0\log(0) + 0\log(0) ] = 0$
	\item $H(X_3) =  -[0,33\log(0,33) + 0,33\log(0,33) + 0,33\log(0,33) ] \approx 1,5848$
	\item $H(X_4) =  -[0,80\log(0,80) + 0,20\log(0,20) + 0\log(0) ] \approx 0,722$
\end{enumerate}

$$
X_1 = X_2 < X_4 < X_3
$$

\subsection{Binary Cross Entropy Loss}

La seguente formula è equivalente alla nostra funzione di Loss, ma fornisce un'interpretazione più intuitiva. Definiamo la \textbf{Binary Cross Entropy Loss} come:

$$
H_{BCE}(X) = \frac{1}{m}\sum_{i=1}^{m} \left(-\sum_{k=1}^{K}\underbrace{p_k^{(i)}}_{\text{(a)}} \log_2(\underbrace{q_k^{(i)}}_{\text{(b)}})\right)
$$

Dove (a) è la distribuzione vera della classificazione, detta \textbf{ground truth}, ovvero quella con probabilità massima sulla classe attesa, mentre (b) è la distribuzione data dal classificatore. Misura quindi la \textbf{distanza tra le due distribuzioni}. Minimizzare $H_{BCE}(X)$, significherà rendere le distribuzioni ideali e quelle effettive del classificatore, quanto più simili possibile.

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.8\linewidth]{images/coss-entropy2}
\end{figure}


\newpage

\section{Overfitting nella classificazione}
Rischiamo di cadere in overfitting nell'utilizzo di classificatori polinomiali. Basterà usare i termini di regolarizzazione per diminuire o annullare l'impatto di alcuni termini di grado eccessivamente alto, ottenendo così:
$$
J(\vartheta) = - \frac{1}{m} \sum_{i=1}^{m}(y^{(i)}\log (h_\vartheta(x)) + (1-y^{(i)})\log (1 - h_\vartheta(x))) + \frac{\lambda}{2m}\sum_{j=1}^n\vartheta_j^2
$$ 

\begin{figure}[tbph]
	\centering
	\includegraphics[width=1\linewidth]{images/underoverfitting-classification}
\end{figure}

\section{Reject Region, regione d'incertezza}

Sia nella classificazione binaria, che in quella multiclasse, è possibile osservare casi in cui la probabilità stimata che un input appartenga ad una tra le classi specificate, non superi un determinato valore di certezza. Sono dei casi limite che vanno gestiti in maniera opportuna:
\begin{enumerate}
	\item Prendere il valore con certezza più alta, se è proprio obbligatorio stabilire una classe, e il task non è particolarmente delicato.
	\item Scartare l'input. Un'opzione migliore in campi più dellicati, come quello medico.
\end{enumerate}

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.47\linewidth]{images/rejection-region}
	\caption{La zona d'incertezza (in cui $P(C_1|x) \sim P(C_2|x)$)}
\end{figure}


\newpage

\section{Classificazione multiclasse, approccio naive OVA e OVO}  
La classificazione multiclasse si distingue da quella binaria per il numero di classi.
Il dataset di input non avrà più etichette binarie, ma $k$ etichette. A ogni classe si associa un numero, ai fini di semplicità. 

\subsection{Metodo One vs All}
Possiamo ottenere una classificazione multiclasse da dei classificatori binari, usando la metodologia \textbf{one vs all}. Immaginiamo di avere un sistema di classificazione figure geometriche, con le classi \textit{triangolo}, \textit{quadrato }e \textit{cerchio }($c_1,c_2,c_3$).
Avremo bisogno di tre classificatori, $h_{\vartheta}^1, h_{\vartheta}^2, h_{\vartheta}^3$, capaci di misurare $P(\text{triangolo}|x), P(\text{quadrato}|x), $ $P(\text{cerchio}|x)$. Prenderemo poi l'etichetta associata al valore di probabilità più alto

$$
\hat{k} = \text{arg}\max_k h_{\vartheta}^k(\overline{x})
$$

ottenendo effettivamente una classificazione multiclasse. 

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.8\linewidth]{images/onevsrest}
\end{figure}

Il training è effettuato sui singoli classificatori binari.
Osserviamo inoltre che, con $k$ classi:

$$
\text{Numero di classificatori richiesti in one vs all} = k 
$$

\subsection{Metodo One vs One}
In questo caso, i classificatori distinguono classi a due a due. Si fanno poi le opportune valutazioni per capire quale classe assegnare. Il numero di classificatori risulta essere più alto. Con $k$ classi abbiamo:

$$
\text{Numero di classificatori richiesti in one vs one} = \frac{k(k-1)}{2}
$$

Immaginiamo un classificatore per lo stesso problema precedentemente esposto: otteniuamo le classi $(c_1,c_2,c_3)$. Questi classificatori su coppie, daranno poi dei risultati. La classe più votata vince:
$$
c_1 \text{ vs } c_2, \quad c_2 \text{ vs } c_3, \quad c_1 \text{ vs } c_3
$$

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.75\linewidth]{images/onevsone}
\end{figure}


\subsection{Confronto tra \textit{One vs All} e \textit{One vs One}}
Il modello one vs one, per quanto più oneroso in termini di numero di classificatori binari richiesti, non presenta l'area di incertezza su tutte le classi, che invece possiamo trovare nel classificatore one vs all.

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.75\linewidth]{images/onevsonevsonevsall}
\end{figure}

\section{Stochastic Gradient Descent}
L'algoritmo di discesa del gradiente, computa $J(\vartheta)$ su \textbf{tutti} i campioni del training set: associando al numero di campioni il valore $m$, a $d$ il numero di feature e ad $e$ le epoche di training, otteniamo un numero di operazioni complessive pari a 

$$
m \times d \times e
$$

Questo prodotto raggiunge facilmente valori molto alti. Un modo per ridurre il numero di operazioni, senza intaccare il risultato del training, e arrivare a convergenza, è l'uso della \textbf{SGD}, ossia della \textbf{discesa del gradiente stocastica}. Ad ogni iterazione del training si campiona un mini-\textbf{batch} di campioni di dimensione $b \leq m$. Questo campione è scelto randomicamente a ogni iterazione.
$$
b \times d \times e
$$

L'algoritmo deve arrivare a convergenza: se non arriva a convergenza, si ripete il campionamento e si ripete il training.

\section{Softmax - Classificazione Multiclasse}
Abbiamo quindi visto com'è possibile implementare un classificatore multiclasse utilizzando più classificatori binari. Tuttavia, questa strategia (a prescindere da OVA e OVO) presenta delle criticità:

\begin{itemize}
	\item Numero di classificatori elevato, che porta a tempi di training lunghi.
	\item Possibili aree di incertezza.
	\item Difficoltà nell'interpretare le probabilità restituite dai classificatori.
\end{itemize}

Si può ovviare a questi problemi con il modello \textbf{Softmax}, che estende la regressione logistica alla classificazione multiclasse.

\subsection{Definizione}
Sia $K$ il numero di classi da prevedere. Il modello Softmax restituisce un vettore $P_\vartheta^{(k)}$ di lunghezza $K$, in cui ogni elemento $P_\vartheta^{(i)}$ rappresenta \textbf{la probabilità} che l'input $x$ appartenga alla classe $i$. L'obiettivo è quindi, cercare di \textbf{stimare}:
\[
P_{\vartheta}^{(k)}(y=k | x) \qquad \forall k \in \{1, \ldots, K\}
\]

la somma di ogni elemento del vettore, è sempre $=1$.
\subsection{Funzione Softmax}
Partiamo dall'idea che il softmax restituisce un vettore di probabilità:
\[
h_{\vartheta}^{(c)}(x) = 
\begin{pmatrix}
P_{\vartheta}^{(1)}(y=1 | x) \\
P_{\vartheta}^{(2)}(y=2 | x) \\
\vdots \\
P_{\vartheta}^{(K)}(y=K | x)
\end{pmatrix}
= 
\frac{1}{\underbrace{\sum_{k=0}^{K-1} e^{\vartheta^{(k) T} x}}_{\text{Normalizzatore della distribuzione}}}
\cdot
\begin{pmatrix}
e^{\vartheta^{(1) T} x} \\
e^{\vartheta^{(2) T} x} \\
\vdots \\
e^{\vartheta^{(k) T} x}
\end{pmatrix}
=
\frac{e^{\vartheta^{(c) T} x}}{\sum_{k=0}^{K-1} e^{\vartheta^{(k) T} x}}
\]

\noindent
Dove $c$ è la classe di cui vogliamo calcolare la probabilità, $k$ è l'indice che scorre tutte le classi, e $\vartheta^{(k)}$ è il vettore dei parametri associati alla classe $k$.

\textbf{N.B.} Ogni classe $k$ ha il proprio vettore di parametri $\vartheta^{(k)}$ (i pesi del modello). Inoltre, una volta finito il learning del modello, la somma delle probabilità restituite dal softmax sarà sempre pari a 1 ($\displaystyle\sum_{k=1}^{K} P_{\vartheta}^{(k)}(y=k | x) = 1$).

\subsection{Funzione di Costo}
La funzione di costo del softmax è una funzione chiamata Cross Entropy Loss. Prima di poter definire la funzione di costo, definiamo una variabile ausiliaria \(1\{\text{proposizione}\} \) che vale 1 se la proposizione è vera, 0 altrimenti.
La funzione di costo del softmax è quindi:
\[
J(\vartheta) = 
- \frac{1}{m} 
\left[
\sum_{i=1}^{m} \sum_{c=1}^{K} 1\{y^{(i)} = c\} \log P_{\vartheta}^{(c)}(y=c | x^{(i)})
\right]
+
\underbrace{\frac{\lambda}{2} \sum_{c=1}^{K} \sum_{j=1}^{n} (\vartheta_j^{(c)})^2}_{\text{Termine di regolarizzazione}}
\]

\noindent
Dove il termine di regolarizzazione serve a prevenire l'overfitting del modello.

\subsection{Obiettivo di Learning}
L'obiettivo di learning è quello di minimizzare la funzione di costo, trovando i parametri ottimali \(\vartheta\):
\[
\vartheta = \arg \min_{\vartheta} J(\vartheta)
\]

Ovvero , trovare i parametri che minimizzano la distanza tra la distribuzione di probabilità predetta dal modello e la distribuzione di probabilità reale (ground truth).

Per fare questo possiamo usare la discesa del gradiente, per cui abbiamo bisogno della derivata parziale della funzione di costo rispetto ai parametri \(\vartheta_j^{(c)}\):
\[
\frac{\partial J(\vartheta)}{\partial \vartheta_j^{(c)}}
=
- \frac{1}{m}
\sum_{i=1}^{m}
\left[
\left(1\{y^{(i)} = c\} - P_{\vartheta}^{(c)}(y=c | x^{(i)})\right)
x_j^{(i)}
\right]
+
\underbrace{\lambda \vartheta_j^{(c)}}_{\text{Termine di regolarizzazione}}
\]

E da questa, aggiorniamo i pesi \(\vartheta_j^{(c)}\) utilizzando la regola di aggiornamento della discesa del gradiente:
\[
\vartheta_j^{(c)} \leftarrow \vartheta_j^{(c)} - \alpha \frac{\partial J(\vartheta)}{\partial \vartheta_j^{(c)}}, \qquad \forall j \in [0, n], \quad \forall c \in [0, K-1]
\]

\subsection{Proprietà del Softmax}
Il modello softmax è \textbf{overparametrizzato}: per ogni ipotesi che potrebbe essere fatta sui dati, esistono infinite combinazioni di parametri che a partire dal vettore di feature $X \in \mathbb{R}^n$ producono la stessa predizione nel vettore di probabilità $Y$.

\noindent
Per dimostrarlo, partiamo da:
\begin{align*}
h_{\vartheta}^{(c)}(x) 
&=\dfrac{e^{\vartheta^{(c)\!\top} x}}
        {\displaystyle\sum_{k=0}^{K-1} e^{\vartheta^{(k)\!\top} x}}
        && \text{Formula del Softmax} \\[4pt]
&=\dfrac{e^{(\vartheta^{(c)}-\psi)^{\!\top} x}}
        {\displaystyle\sum_{k=0}^{K-1} e^{(\vartheta^{(k)}-\psi)^{\!\top} x}}
        && \text{Sottraiamo lo stesso $\psi$} \\[4pt]
&=\dfrac{e^{\vartheta^{(c)\!\top} x}\,e^{-\psi^{\!\top} x}}
        {\displaystyle\sum_{k=0}^{K-1} e^{\vartheta^{(k)\!\top} x}\,e^{-\psi^{\!\top} x}}
        && \text{Separiamo gli esponenziali} \\[4pt]
&=\dfrac{e^{\vartheta^{(c)\!\top} x}}
        {\displaystyle\sum_{k=0}^{K-1} e^{\vartheta^{(k)\!\top} x}}
        && \text{Si cancella il fattore comune $e^{-\psi^{\!\top} x}$}
\end{align*}

\noindent
Dove $\psi \in \mathbb{R}^n$ è un vettore arbitrario. Quindi, sottraendo lo stesso vettore $\psi$ da tutti i vettori di parametri $\vartheta^{(k)}$, otteniamo gli stessi valori di probabilità predetti dal modello, perciò esistono infinite combinazioni di parametri che producono la stessa predizione.

\paragraph{Generalizzazione funzione di costo.}
É facile vedere che, per $K=2$ (senza regolarizzazione) vale:
\begin{align*}
J(\vartheta)
&= -\frac{1}{m}\sum_{i=1}^{m}\sum_{c=0}^{1}
\mathbf{1}\{y^{(i)}=c\}\,\log\!\big(h_{\vartheta}^{(c)}(x^{(i)})\big)\\
&= -\frac{1}{m}\sum_{i=1}^{m}
\Big[\mathbf{1}\{y^{(i)}=0\}\log h_{\vartheta}^{(0)}(x^{(i)})
     +\mathbf{1}\{y^{(i)}=1\}\log h_{\vartheta}^{(1)}(x^{(i)})\Big]\\
&= -\frac{1}{m}\sum_{i=1}^{m}
\Big[(1-y^{(i)})\log h_{\vartheta}^{(0)}(x^{(i)})
     +y^{(i)}\log h_{\vartheta}^{(1)}(x^{(i)})\Big]\\
&= -\frac{1}{m}\sum_{i=1}^{m}
\Big[(1-y^{(i)})\log\!\big(1-h_{\vartheta}^{(1)}(x^{(i)})\big)
     +y^{(i)}\log h_{\vartheta}^{(1)}(x^{(i)})\Big],
\end{align*}

dove si è usata la normalizzazione della softmax $h_{\vartheta}^{(0)}(x)+h_{\vartheta}^{(1)}(x)=1$.
Questa è esattamente la loss della regressione logistica binaria: dunque la regressione logistica è un caso particolare della softmax.

Si può ulteriormente dimostrare che il softmax è una generalizzazione della regressione logistica multiclasse usando una sua proprietà principale: l'\emph{overparametrizzazione}. Per $K=2$:
\[
h_{\vartheta}(x)=
\begin{bmatrix}
\dfrac{e^{\vartheta^{(0)\!\top}x}}{e^{\vartheta^{(0)\!\top}x}+e^{\vartheta^{(1)\!\top}x}}\\[6pt]
\dfrac{e^{\vartheta^{(1)\!\top}x}}{e^{\vartheta^{(0)\!\top}x}+e^{\vartheta^{(1)\!\top}x}}
\end{bmatrix}
=
\begin{bmatrix}
\dfrac{e^{(\vartheta^{(0)}-\vartheta^{(1)})^\top x}}{e^{(\vartheta^{(0)}-\vartheta^{(1)})^\top x}+e^{\mathbf 0^\top x}}\\[6pt]
\dfrac{e^{(\vartheta^{(1)}-\vartheta^{(1)})^\top x}}{e^{(\vartheta^{(0)}-\vartheta^{(1)})^\top x}+e^{\mathbf 0^\top x}}
\end{bmatrix}
=
\begin{bmatrix}
\dfrac{e^{\Delta^\top x}}{1+e^{\Delta^\top x}}\\[6pt]
\dfrac{1}{1+e^{\Delta^\top x}}
\end{bmatrix},
\qquad \Delta=\vartheta^{(0)}-\vartheta^{(1)}.
\]
Ponendo \(\mathbf w=\vartheta^{(1)}-\vartheta^{(0)}=-\Delta\) e
\(\sigma(z)=\dfrac{1}{1+e^{-z}}\) si ottiene
\[
h_{\vartheta}(x)=
\begin{bmatrix}
1-\sigma(\mathbf w^\top x)\\[4pt]
\sigma(\mathbf w^\top x)
\end{bmatrix}.
\]

\noindent
Quindi, per $K=2$, il softmax riduce alla regressione logistica binaria.

\subsection{Rete neurale Softmax}
Il softmax può essere visto come un particolare tipo di rete neurale:
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/softmax_neural_network.png}
	\caption{Flusso della regressione softmax con due ingressi e tre classi: dai logit
	$z_i = w_{i,1}x_1 + w_{i,2}x_2 + b_i$ alle probabilità
	$a_i=\frac{e^{z_i}}{\sum_{k=1}^{3} e^{z_k}}$, fino alla loss a entropia incrociata
	$L = \sum_{j=1}^{3} -y_j \log(a_j)$.}	
	\label{fig:softmax_neural_network}
\end{figure}

\subsection{Funzione di Loss}
La funzione di Loss usata nel softmax normalmente è una variante di \textbf{cross entropy loss} (molto simile alla funzione di costo). Si possono usare alternative, in particolare la \textbf{Hinge Loss} trasforma la classificazione multiclasse in un problema di massimizzazione del margine tra le classi, come nelle SVM. La Hinge Loss è definita come:
\[
\text{Loss}(h_\vartheta(x), y) = \sum_{i \
neq y} \max(0, h_\vartheta(x)_i - h_\vartheta(x)_y + \Delta)
\]
