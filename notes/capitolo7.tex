\chapter{Decision Tree}
Gli alberi decisionali (decision tree) sono modelli utilizzati per la classificazione e la regressione. Essi rappresentano un insieme di regole decisionali organizzate in una struttura ad albero, dove ogni nodo interno rappresenta una condizione su una caratteristica dei dati, ogni ramo rappresenta l'esito della condizione e ogni foglia rappresenta una classe o un'etichetta di output.

\section{Alberi di classificazione}
Un albero decisionale per la classificazione è una struttura gerarchica composta da nodi interni e foglie. Ogni nodo interno rappresenta un test su una caratteristica specifica del dataset, mentre ogni foglia rappresenta una classe di output. L'obiettivo principale di un albero decisionale è suddividere il dataset in modo tale che le tuple appartenenti alla stessa classe siano raggruppate insieme nelle foglie dell'albero.

Questo approccio, comunemente chiamato \textbf{divide and conquer}, suddivide iterativamente il dataset in sottoinsiemi più piccoli basati su condizioni specifiche, fino a quando non si raggiungono le foglie dell'albero che rappresentano le decisioni finali.

\subsection{Divisione ricorsiva}
La classe di una tupla $q$ si ottiene seguendo il cammino radice $\rightarrow$ foglia guidato dai blocchi condizionali sui nodi interni. Ogni nodo interno applica un test su una caratteristica $A$ e, in base al risultato del test, si procede lungo il ramo corrispondente fino a raggiungere una foglia che fornisce la classificazione finale. L'insieme di regole \emph{deve} essere \textbf{esaustivo} e \textbf{mutuamente esclusivo}\footnote{Se così non fosse, alcune tuple potrebbero non essere classificate o potrebbero essere classificate da più regole contemporaneamente} (ogni tupla deve essere classificata da una e una sola regola).

La costruzione dell'albero decisionale è molto semplice:
\begin{enumerate}
  \item Se tutte le tuple del nodo $X$ hanno la \emph{stessa} classe $C$, crea una foglia $C$.
  \item Altrimenti scegli un attributo $A$ (non ancora usato) e \emph{ramifica} $X$ (\emph{splitting}) secondo i valori/soglia di $A$; crea i figli.
  \item Per ogni figlio $X_i$: se puro, fermati; se impuro, ripeti ricorsivamente.
\end{enumerate}

\paragraph{Pruning.}
Se le tuple nel nodo sono poche o la profondità è elevata, si può fermare prima e rendere il nodo una foglia (vedere figura \ref{fig:weather-tree} con l'attributo "overcast").

\begin{figure}[htbp]
  \centering
  \begin{minipage}[t]{.50\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/weather_table.png}
  \end{minipage}\hfill
  \begin{minipage}[t]{.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/decision_tree_weather.png}
  \end{minipage}
  \caption{Dataset \emph{weather} (a sinistra) e albero decisionale appreso (a destra). 
  La tabella contiene 14 esempi con quattro attributi descrittivi (\texttt{Outlook}, \texttt{Temperature}, 
  \texttt{Humidity}, \texttt{Windy}) e la classe binaria \texttt{P/N}. 
  L’albero (stile ID3/C4.5) sceglie come radice \texttt{Outlook}; il ramo \texttt{overcast} porta 
  direttamente alla classe \texttt{P}, mentre per \texttt{sunny} si testa \texttt{Humidity} e per \texttt{rain} 
  si testa \texttt{Windy}. L’esempio illustra il passaggio da dati tabellari a regole interpretabili.}
  \label{fig:weather-tree}
\end{figure}

\subsection{Migliore divisione dei nodi}
Diciamo che per il nodo $m$, $N_m$ è il numero di tuple nel training set che raggiungono il nodo $m$ (per la radice, $N_{root} = N$, il numero totale di tuple). Sia $N_m^i$ il numero di tuple di classe $C_i$ che raggiungono il nodo $m$, con $\sum_i N_m^i = N_m$. Sapendo che una tupla $x$ raggiunge il nodo $m$, la probabilità stimata che appartenga alla classe $C_i$ è:
\[
\hat{P}(C_i | x, m) \equiv p_m^i = \frac{N_m^i}{N_m}
\]

Definiamo un \textbf{nodo puro} come un nodo in cui tutte le tuple appartengono alla stessa classe:
\[
\exists i : N_m^i = N_m \implies p_m^i = 1 \quad \text{e} \quad \forall j \neq i : p_m^j = 0
\]
ovvero la probabilità stimata di una classe è 1, mentre per tutte le altre è 0. Un nodo è \textbf{impuro} se contiene tuple di classi diverse. Un modo frequente per misurare l'impurità di un nodo è utilizzare l'entropia di Shannon:
\[
H(m) = - \sum_{i} p_m^i \log_2(p_m^i)
\]
L'entropia misura l'incertezza associata alla distribuzione delle classi nel nodo. Un nodo puro ha \emph{entropia zero}, mentre un nodo con una distribuzione uniforme delle classi ha \emph{entropia massima}.

Per trovare la miglior combinazione dei nodi eseguiamo la seguente procedura ricorsiva:
\begin{itemize}
    \item Se il nodo è \emph{puro}, creiamo una foglia con la classe corrispondente.
    \item Altrimenti, per ogni attributo $A$ non ancora usato, si divide.
\end{itemize}

Per calcolare l'impurità dei nodi dopo lo splitting (un nodo impuro) bisogna diminuire il valore di impurità. Per calcolarlo:
\[
\hat{P}(C_i | x, m, j) \equiv p_{mj}^i = \frac{N_{mj}^i}{N_{mj}}
\]
dove $N_{mj}$ è il numero di tuple che raggiungono il figlio $j$ del nodo $m$ e $N_{mj}^i$ è il numero di tuple di classe $C_i$ che raggiungono il figlio $j$ del nodo $m$. Questo si traduce in una nuova entropia calcolata come:
\[
H^j(m) = - \sum_{j=1}^n \frac{N_{mj}}{N_m} \sum_{i}^k p_{mj}^i \log_2(p_{mj}^i)
\]
dove $n$ è il numero di figli del nodo $m$ e $k$ è il numero di classi. L'entropia dopo lo splitting è una media pesata delle entropie dei figli, ponderata per la proporzione di tuple che raggiungono ciascun figlio. 

\section{Alberi di regressione}
Un albero di regressione è un modello predittivo utilizzato per stimare valori continui basati su un insieme di caratteristiche. A differenza degli alberi decisionali per la classificazione, che suddividono i dati in classi discrete, gli alberi di regressione suddividono i dati in intervalli continui e forniscono una stima numerica come output. 

\subsection{Costruzione dell'albero di regressione}
Ipotizziamo di avere un nodo $m$, con $\mathcal{X}_m$ che rappresenta l'insieme $\mathcal{X}$ delle tuple che raggiungono il nodo $m$. Quindi è l'insieme di tutti quei nodi $x$ tali che $x \in \mathcal{X}$ e $x$ raggiunge il nodo $m$. Possiamo definire:
\[
b_m(x) = 
\begin{cases}
1 & \text{se } x \in \mathcal{X}_m \\
0 & \text{altrimenti}
\end{cases}
\]
ovvero una funzione indicatrice che vale 1 se la tupla $x$ raggiunge il nodo $m$ e 0 altrimenti.

Nella regressione, la \emph{goodness}\footnote{Qualità} di una divisione viene misurata utilizzando la somma dei quadrati degli errori (SSE, Sum of Squared Errors). Per un nodo $m$, la SSE è definita come:
\[
E_m = \frac{1}{N_m} \sum_{t} (y^t - \hat{y}_m)^2 \, b_m(x^t)
\]
dove $y^t$ è il valore reale della tupla $t$, $\hat{y}_m$ è la stima associata al nodo $m$, e $N_m$ è il numero di tuple che raggiungono il nodo $m$. Si moltiplica per la funzione indicatrice $b_m(x^t)$ per considerare solo le tuple che effettivamente raggiungono il nodo $m$. La stima media $\hat{y}_m$ per il nodo $m$ è calcolata come:
\[
\hat{y}_m = \frac{\sum_{t} b_m(x^t)\, y^t}{\sum_{t} b_m(x^t)} = \frac{1}{N_m} \sum_{t} b_m(x^t)\, y^t
\]
dove la somma è calcolata su tutte le tuple $t$ che raggiungono il nodo $m$. In pratica, $\hat{y}_m$ è la media dei valori di output delle tuple che raggiungono il nodo $m$.

\subsection{Divisione di un nodo}
Per dividere un nodo $m$, si seleziona un attributo $A$ e una soglia di divisione. La divisione crea più figli per il nodo $m$, ciascuno corrispondente a un intervallo di valori dell'attributo $A$. Dopo la divisione, si calcola l'errore $E_m^j$ per ciascun figlio $j$ del nodo $m$. 

Se l'errore $E_m$ viene ritenuto accettabile, ovvero $E_m < \theta_r$ per una certa soglia, si crea una foglia con la stima $\hat{y}_m$. Altrimenti, se l'errore non è accettabile, si procede viene diviso tante volte finché non si raggiunge una stima accettabile o si esauriscono gli attributi disponibili per la divisione. Si definisce $\mathcal{X}_{mj}$ come l'insieme delle tuple che raggiungono il figlio $j$ del nodo $m$ sottoinsieme di $\mathcal{X}_m$. Definiamo ulteriormente:
\[
b_{mj}(x) =
\begin{cases}
1 & \text{se } x \in \mathcal{X}_{mj} \\
0 & \text{altrimenti}
\end{cases}
\]
La stima media per il figlio $j$ del nodo $m$ è calcolata come:
\[
\hat{y}_{mj} = \frac{\sum_{t} b_{mj}(x^t)\, y^t}{\sum_{t} b_{mj}(x^t)} = \frac{1}{N_{mj}} \sum_{t} b_{mj}(x^t)\, y^t
\]
dove $N_{mj}$ è il numero di tuple che raggiungono il figlio $j$ del nodo $m$. L'errore dopo lo split è calcolato come:
\[
E^j_m = \frac{1}{N_m} \sum_{t} \sum_{j} (y^t - \hat{y}_{mj})^2 \, b_{mj}(x^t)
\]
dove la somma esterna è calcolata su tutte le tuple $t$ che raggiungono il nodo $m$ e la somma interna è calcolata su tutti i figli $j$ del nodo $m$. Ipotizziamo di avere un nodo m, con Xm che rappresenta l'insieme X delle tuple che raggiungono il nodo m. Quindi è l'insieme di tutti quei nodi $x$ tali che $x \in X$ e $x$ raggiunge il nodo $m$.

\section{Pruning: riduzione dell'albero}
Il pruning è una tecnica utilizzata per ridurre la complessità di un albero decisionale (di classificazione) o di regressione, al fine di migliorare la sua capacità di generalizzazione sui dati non visti. Durante la costruzione dell'albero, è possibile che si creino nodi che si adattano troppo strettamente ai dati di addestramento, portando ad \emph{overftting}. Il pruning mira a rimuovere questi nodi superflui, semplificando l'albero e migliorando le prestazioni sui dati di test.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\textwidth]{images/regression_tree_thresholds.png}
    \caption{Esempio di albero di regressione in cui si mostra l'effetto del parametro di complessità $\theta_r$ sulla funzione stimata: al diminuire di $\theta_r$ l'albero produce più suddivisioni e una funzione a gradini più irregolare e aderente ai dati.}
    \label{fig:regression_tree_theta}
\end{figure}

\subsection{Pre-pruning}
Il pre-pruning consiste nell'interrompere la crescita dell'albero prima che raggiunga la sua massima profondità. Durante la costruzione dell'albero, si valuta la bontà di ogni divisione e si decide di fermarsi se la divisione non migliora significativamente le prestazioni del modello. Questo può essere fatto utilizzando criteri come la riduzione dell'impurità o la diminuzione dell'errore di regressione.

\subsection{Post-pruning}
Il post-pruning, invece, avviene dopo che l'albero è stato completamente costruito. In questa fase, si esaminano i nodi dell'albero e si valutano le prestazioni del modello su un set di validazione. I nodi che non contribuiscono significativamente alla precisione del modello vengono rimossi, in quanto contribuiscono all'\emph{overfitting}, e i loro figli vengono sostituiti con foglie che rappresentano la stima media o la classe più frequente delle tuple che raggiungevano quel nodo.

\section{Regole di learning}
Generalmente il metodo IF-THEN funziona per addestrare un albero decisional e dopo convertire questo in regole. Altri metodi riguardano l'apprendimento sulle \textbf{regole stesse}.

\subsection{Introduzione di regole}
In questo approccio 