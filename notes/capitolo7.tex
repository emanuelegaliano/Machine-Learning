\chapter{Decision Tree}
Gli alberi decisionali (decision tree) sono modelli utilizzati per la classificazione e la regressione. Essi rappresentano un insieme di regole decisionali organizzate in una struttura ad albero, dove ogni nodo interno rappresenta una condizione su una caratteristica dei dati, ogni ramo rappresenta l'esito della condizione e ogni foglia rappresenta una classe o un'etichetta di output.

\section{Alberi di classificazione}
Un albero decisionale per la classificazione è una struttura gerarchica composta da nodi interni e foglie. Ogni nodo interno rappresenta un test su una caratteristica specifica del dataset, mentre ogni foglia rappresenta una classe di output. L'obiettivo principale di un albero decisionale è suddividere il dataset in modo tale che le tuple appartenenti alla stessa classe siano raggruppate insieme nelle foglie dell'albero.

Questo approccio, comunemente chiamato \textbf{divide and conquer}, suddivide iterativamente il dataset in sottoinsiemi più piccoli basati su condizioni specifiche, fino a quando non si raggiungono le foglie dell'albero che rappresentano le decisioni finali.

\subsection{Divisione ricorsiva}
La classe di una tupla $q$ si ottiene seguendo il cammino radice $\rightarrow$ foglia guidato dai blocchi condizionali sui nodi interni. Ogni nodo interno applica un test su una caratteristica $A$ e, in base al risultato del test, si procede lungo il ramo corrispondente fino a raggiungere una foglia che fornisce la classificazione finale. L'insieme di regole \emph{deve} essere \textbf{esaustivo} e \textbf{mutuamente esclusivo}\footnote{Se così non fosse, alcune tuple potrebbero non essere classificate o potrebbero essere classificate da più regole contemporaneamente} (ogni tupla deve essere classificata da una e una sola regola).

La costruzione dell'albero decisionale è molto semplice:
\begin{enumerate}
  \item Se tutte le tuple del nodo $X$ hanno la \emph{stessa} classe $C$, crea una foglia $C$.
  \item Altrimenti scegli un attributo $A$ (non ancora usato) e \emph{ramifica} $X$ (\emph{splitting}) secondo i valori/soglia di $A$; crea i figli.
  \item Per ogni figlio $X_i$: se puro, fermati; se impuro, ripeti ricorsivamente.
\end{enumerate}

\paragraph{Pruning.}
Se le tuple nel nodo sono poche o la profondità è elevata, si può fermare prima e rendere il nodo una foglia (vedere figura \ref{fig:weather-tree} con l'attributo "overcast").

\begin{figure}[htbp]
  \centering
  \begin{minipage}[t]{.50\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/weather_table.png}
  \end{minipage}\hfill
  \begin{minipage}[t]{.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/decision_tree_weather.png}
  \end{minipage}
  \caption{Dataset \emph{weather} (a sinistra) e albero decisionale appreso (a destra). 
  La tabella contiene 14 esempi con quattro attributi descrittivi (\texttt{Outlook}, \texttt{Temperature}, 
  \texttt{Humidity}, \texttt{Windy}) e la classe binaria \texttt{P/N}. 
  L’albero (stile ID3/C4.5) sceglie come radice \texttt{Outlook}; il ramo \texttt{overcast} porta 
  direttamente alla classe \texttt{P}, mentre per \texttt{sunny} si testa \texttt{Humidity} e per \texttt{rain} 
  si testa \texttt{Windy}. L’esempio illustra il passaggio da dati tabellari a regole interpretabili.}
  \label{fig:weather-tree}
\end{figure}

\subsection{Migliore divisione dei nodi}
Diciamo che per il nodo $m$, $N_m$ è il numero di tuple nel training set che raggiungono il nodo $m$ (per la radice, $N_{root} = N$, il numero totale di tuple). Sia $N_m^i$ il numero di tuple di classe $C_i$ che raggiungono il nodo $m$, con $\sum_i N_m^i = N_m$. Sapendo che una tupla $x$ raggiunge il nodo $m$, la probabilità stimata che appartenga alla classe $C_i$ è:
\[
\hat{P}(C_i | x, m) \equiv p_m^i = \frac{N_m^i}{N_m}
\]

Definiamo un \textbf{nodo puro} come un nodo in cui tutte le tuple appartengono alla stessa classe:
\[
\exists i : N_m^i = N_m \implies p_m^i = 1 \quad \text{e} \quad \forall j \neq i : p_m^j = 0
\]
ovvero la probabilità stimata di una classe è 1, mentre per tutte le altre è 0. Un nodo è \textbf{impuro} se contiene tuple di classi diverse. Un modo frequente per misurare l'impurità di un nodo è utilizzare l'entropia di Shannon:
\[
H(m) = - \sum_{i} p_m^i \log_2(p_m^i)
\]
L'entropia misura l'incertezza associata alla distribuzione delle classi nel nodo. Un nodo puro ha \emph{entropia zero}, mentre un nodo con una distribuzione uniforme delle classi ha \emph{entropia massima}.

Per trovare la miglior combinazione dei nodi eseguiamo la seguente procedura ricorsiva:
\begin{itemize}
    \item Se il nodo è \emph{puro}, creiamo una foglia con la classe corrispondente.
    \item Altrimenti, per ogni attributo $A$ non ancora usato, si divide.
\end{itemize}

Per calcolare l'impurità dei nodi dopo lo splitting (un nodo impuro) bisogna diminuire il valore di impurità. Per calcolarlo:
\[
\hat{P}(C_i | x, m, j) \equiv p_{mj}^i = \frac{N_{mj}^i}{N_{mj}}
\]
dove $N_{mj}$ è il numero di tuple che raggiungono il figlio $j$ del nodo $m$ e $N_{mj}^i$ è il numero di tuple di classe $C_i$ che raggiungono il figlio $j$ del nodo $m$. Questo si traduce in una nuova entropia calcolata come:
\[
H^j(m) = - \sum_{j=1}^n \frac{N_{mj}}{N_m} \sum_{i}^k p_{mj}^i \log_2(p_{mj}^i)
\]
dove $n$ è il numero di figli del nodo $m$ e $k$ è il numero di classi. L'entropia dopo lo splitting è una media pesata delle entropie dei figli, ponderata per la proporzione di tuple che raggiungono ciascun figlio. 

\section{Alberi di regressione}
Un albero di regressione è un modello predittivo utilizzato per stimare valori continui basati su un insieme di caratteristiche. A differenza degli alberi decisionali per la classificazione, che suddividono i dati in classi discrete, gli alberi di regressione suddividono i dati in intervalli continui e forniscono una stima numerica come output. 

\subsection{Costruzione dell'albero di regressione}
Ipotizziamo di avere un nodo $m$, con $\mathcal{X}_m$ che rappresenta l'insieme $\mathcal{X}$ delle tuple che raggiungono il nodo $m$. Quindi è l'insieme di tutti quei nodi $x$ tali che $x \in \mathcal{X}$ e $x$ raggiunge il nodo $m$. Possiamo definire:
\[
b_m(x) = 
\begin{cases}
1 & \text{se } x \in \mathcal{X}_m \\
0 & \text{altrimenti}
\end{cases}
\]
ovvero una funzione indicatrice che vale 1 se la tupla $x$ raggiunge il nodo $m$ e 0 altrimenti.

Nella regressione, la \emph{goodness}\footnote{Qualità} di una divisione viene misurata utilizzando la somma dei quadrati degli errori (SSE, Sum of Squared Errors). Per un nodo $m$, la SSE è definita come:
\[
E_m = \frac{1}{N_m} \sum_{t} (y^t - \hat{y}_m)^2 \, b_m(x^t)
\]
dove $y^t$ è il valore reale della tupla $t$, $\hat{y}_m$ è la stima associata al nodo $m$, e $N_m$ è il numero di tuple che raggiungono il nodo $m$. Si moltiplica per la funzione indicatrice $b_m(x^t)$ per considerare solo le tuple che effettivamente raggiungono il nodo $m$. La stima media $\hat{y}_m$ per il nodo $m$ è calcolata come:
\[
\hat{y}_m = \frac{\sum_{t} b_m(x^t)\, y^t}{\sum_{t} b_m(x^t)} = \frac{1}{N_m} \sum_{t} b_m(x^t)\, y^t
\]
dove la somma è calcolata su tutte le tuple $t$ che raggiungono il nodo $m$. In pratica, $\hat{y}_m$ è la media dei valori di output delle tuple che raggiungono il nodo $m$.

\subsection{Divisione di un nodo}
Per dividere un nodo $m$, si seleziona un attributo $A$ e una soglia di divisione. La divisione crea più figli per il nodo $m$, ciascuno corrispondente a un intervallo di valori dell'attributo $A$. Dopo la divisione, si calcola l'errore $E_m^j$ per ciascun figlio $j$ del nodo $m$. 

Se l'errore $E_m$ viene ritenuto accettabile, ovvero $E_m < \theta_r$ per una certa soglia, si crea una foglia con la stima $\hat{y}_m$. Altrimenti, se l'errore non è accettabile, si procede viene diviso tante volte finché non si raggiunge una stima accettabile o si esauriscono gli attributi disponibili per la divisione. Si definisce $\mathcal{X}_{mj}$ come l'insieme delle tuple che raggiungono il figlio $j$ del nodo $m$ sottoinsieme di $\mathcal{X}_m$. Definiamo ulteriormente:
\[
b_{mj}(x) =
\begin{cases}
1 & \text{se } x \in \mathcal{X}_{mj} \\
0 & \text{altrimenti}
\end{cases}
\]
La stima media per il figlio $j$ del nodo $m$ è calcolata come:
\[
\hat{y}_{mj} = \frac{\sum_{t} b_{mj}(x^t)\, y^t}{\sum_{t} b_{mj}(x^t)} = \frac{1}{N_{mj}} \sum_{t} b_{mj}(x^t)\, y^t
\]
dove $N_{mj}$ è il numero di tuple che raggiungono il figlio $j$ del nodo $m$. L'errore dopo lo split è calcolato come:
\[
E^j_m = \frac{1}{N_m} \sum_{t} \sum_{j} (y^t - \hat{y}_{mj})^2 \, b_{mj}(x^t)
\]
dove la somma esterna è calcolata su tutte le tuple $t$ che raggiungono il nodo $m$ e la somma interna è calcolata su tutti i figli $j$ del nodo $m$. Ipotizziamo di avere un nodo m, con Xm che rappresenta l'insieme X delle tuple che raggiungono il nodo m. Quindi è l'insieme di tutti quei nodi $x$ tali che $x \in X$ e $x$ raggiunge il nodo $m$.

\section{Pruning: riduzione dell'albero}
Il pruning è una tecnica utilizzata per ridurre la complessità di un albero decisionale (di classificazione) o di regressione, al fine di migliorare la sua capacità di generalizzazione sui dati non visti. Durante la costruzione dell'albero, è possibile che si creino nodi che si adattano troppo strettamente ai dati di addestramento, portando ad \emph{overftting}. Il pruning mira a rimuovere questi nodi superflui, semplificando l'albero e migliorando le prestazioni sui dati di test.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\textwidth]{images/regression_tree_thresholds.png}
    \caption{Esempio di albero di regressione in cui si mostra l'effetto del parametro di complessità $\theta_r$ sulla funzione stimata: al diminuire di $\theta_r$ l'albero produce più suddivisioni e una funzione a gradini più irregolare e aderente ai dati.}
    \label{fig:regression_tree_theta}
\end{figure}

\subsection{Pre-pruning}
Il pre-pruning consiste nell'interrompere la crescita dell'albero prima che raggiunga la sua massima profondità. Durante la costruzione dell'albero, si valuta la bontà\footnote{La bontà indica la qualità di misura.} di ogni divisione e si decide di fermarsi se la divisione non migliora significativamente le prestazioni del modello. Questo può essere fatto utilizzando criteri come la riduzione dell'impurità o la diminuzione dell'errore di regressione.

\subsection{Post-pruning}
Il post-pruning, invece, avviene dopo che l'albero è stato completamente costruito. In questa fase, si esaminano i nodi dell'albero e si valutano le prestazioni del modello su un set di validazione. I nodi che non contribuiscono significativamente alla precisione del modello vengono rimossi, in quanto contribuiscono all'\emph{overfitting}, e i loro figli vengono sostituiti con foglie che rappresentano la stima media o la classe più frequente delle tuple che raggiungevano quel nodo.

\section{Regole di learning}
Generalmente il metodo IF-THEN funziona per addestrare un albero decisional e dopo convertire questo in regole. Altri metodi riguardano l'apprendimento sulle \textbf{regole stesse}: invece di costruire un albero e poi estrarre le regole, si \emph{imparano} le regole dai dati e dopo si costruisce l'albero, procedura nota come \textbf{rule induction}.

\subsection{Rule induction}
Una regola è generalmente espressa nella forma:
\[
\text{IF } \text{condition(s)} \text{ THEN } \text{conclusion}
\]
dove le \emph{condition(s)} sono un insieme di test su attributi e la \emph{conclusion} è una classe (per classificazione) o una stima (per regressione). L'obiettivo dell'apprendimento delle regole è identificare un insieme di regole che descrivano accuratamente i dati di addestramento.

\paragraph{Sequential Covering.}
Le \emph{condition} possono essere combinate utilizzando operatori logici come AND e OR per formare regole più complesse. Ad esempio, una regola potrebbe essere:
\[
\text{IF } (A_1 > 5) \text{ AND } (A_2 = 'Yes') \text{ THEN } \text{Class = Positive}
\]
e si aggiungono \textbf{una alla volta} per ottimizzare una certa \emph{euristica} (per esempio l'entropia o la riduzione dell'errore). Nell'esempio di prima, si inizia con la prima regola $R_1 = A_1 > 5$, si valuta la bontà di $R_11$ e si continua ad aggiungere condizioni finché non si raggiunge un certo criterio di arresto (ad esempio, la regola copre un numero sufficiente di tuple positive senza coprire troppe tuple negative). Una volta che una regola è stata appresa, le tuple coperte da quella regola vengono rimosse dal dataset e il processo continua con le tuple rimanenti fino a quando tutte le tuple sono state coperte o non è possibile apprendere ulteriori regole utili.

\paragraph{Differenza con alberi decisionali.}
Mentre gli alberi decisionali suddividono i dati in modo gerarchico, le regole di apprendimento si concentrano sull'identificazione di condizioni specifiche che portano a conclusioni. Le regole possono essere più flessibili e interpretabili rispetto agli alberi decisionali, poiché possono essere applicate in modo indipendente l'una dall'altra. Si parla infatti di:
\begin{description}
  \item[Breadth-First] Questo approccio, utilizzato dagli alberi decisionali, esplora tutte le possibili divisioni, in modo parallelo, ad ogni livello dell'albero prima di procedere al livello successivo (quindi cresce in \emph{larghezza}). 
  \item[Depth-First] Questo approccio, utilizzato dalle regole di apprendimento, esplora una singola divisione alla volta, procedendo in profondità lungo un percorso specifico prima di tornare indietro e esplorare altre divisioni (quindi cresce in \emph{profondità}).
\end{description}


\subsection{Copertura delle regole}
Si dice che una regole \textbf{copre} una tupla $t$ se le condizioni della regola sono soddisfatte dalla tupla $t$. L'insieme delle tuple coperte da una regola è chiamato \textbf{coverage} (copertura) della regola. La copertura aiuta a \textbf{valutare} l'efficiacia di una regola: una regola con una copertura elevata è generalmente considerata più utile, poiché si applica a un numero maggiore di tuple nel dataset.

\paragraph{Ripper.}
Un esempio può essere Ripper (Repeated Incremental Pruning to Produce Error Reduction), un algoritmo di apprendimento delle regole che utilizza una strategia di copertura sequenziale per generare un insieme di regole a partire dai dati di addestramento. Ripper:
\begin{enumerate}
  \item Inizia con un insieme vuoto di regole $\mathcal{R} = \emptyset$.
  \item Ripete fino a quando tutte le tuple positive sono coperte:
  \begin{enumerate}
    \item Crea una nuova regola $R$ iniziando con una condizione vuota.
    \item Aggiunge condizioni a $R$ finché la regola non soddisfa un criterio di arresto (ad esempio, copre un numero sufficiente di tuple positive senza coprire troppe tuple negative).
    \item Aggiunge la regola $R$ all'insieme delle regole $\mathcal{R}$.
    \item Rimuove le tuple coperte da $R$ dal dataset.
  \end{enumerate}
\end{enumerate}

\section{Alberi decisionali multivariati}
Nel caso di alberi decisionali univariati, ogni nodo interno effettua un testo su un singolo attributo alla volta. Si conisdera quindi una funzione del tipo:
\[
f(x) = 
\begin{cases}
\text{figlio destro} & \text{se } A_i \leq \theta \\
\text{figlio sinistro} & \text{se } A_i > \theta
\end{cases}
\]
dove $A_i$ è un singolo attributo e $\theta$ è una soglia. Questo approccio è semplice e facile da interpretare, ma potrebbe non catturare tutte le relazioni complesse tra gli attributi.

\subsection{Alberi decisionali multivariati lineari}
In alcuni casi, potrebbe essere vantaggioso considerare più attributi contemporaneamente per prendere decisioni più complesse, portando alla creazione di \emph{alberi decisionali multivariati lineari} dove si considera una \textbf{combinazione lineare} di attributi in ogni nodo:
\[
f_m(x) = \mathbf{w}_m^T \cdot x + w_{m0} > 0
\]
dove $\mathbf{w}_m$ è un vettore di pesi associati agli attributi, $x$ è il vettore delle caratteristiche della tupla, e $w_{m0}$ è un termine di bias. In questo caso, la decisione su quale figlio seguire dipende dalla valutazione di una combinazione lineare di tutti gli attributi, piuttosto che da un singolo attributo. Questo consente di catturare relazioni più complesse tra gli attributi e può portare a una migliore performance del modello in alcuni scenari.

\subsection{Interpretazione geometrica}
L'uso di combinazioni lineari di attributi in ogni nodo può essere interpretato geometricamente come la suddivisione dello spazio delle caratteristiche in regioni definite da iperpiani. In particolare, la funzione di decisione $f_m(x)$ definisce un iperpiano nello spazio delle caratteristiche, che separa lo spazio in due metà: una metà in cui la condizione è vera (portando al figlio destro) e l'altra metà in cui la condizione è falsa (portando al figlio sinistro) (figura \ref{fig:multivariate_decision_tree}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.8\textwidth]{images/multivariate_decision_tree.png}
    \caption{Esempio di nodo multivariato: a sinistra, gli esempi delle due classi $C_1$ (cerchi) e $C_2$ (quadrati) vengono separati da un iperpiano lineare $dove\; w_{11}x_1 + w_{12}x_2 + w_{10} = 0$. A destra è mostrato il corrispondente nodo dell'albero di decisione, che instrada gli esempi in base al test 
    $w_{11}x_1 + w_{12}x_2 + w_{10} > 0$.}
    \label{fig:multivariate_decision_tree}
\end{figure}

\subsection{Alberi decisionali multivariati non lineari}
Il concetto di alberi decisionali multivariati può essere esteso ulteriormente per includere funzioni non lineari nei nodi. In questo caso, invece di utilizzare una combinazione lineare di attributi, si possono utilizzare funzioni non lineari più complesse, come polinomi, funzioni radiali o reti neurali. Ad esempio, un nodo potrebbe utilizzare una funzione del tipo:
\[
f_m(x): x^T \mathbf{W}_m x + \mathbf{w}_m^T x + w_{m0} > 0
\]
dove $\mathbf{W}_m$ è una matrice di pesi che definisce una funzione quadratica. Questo approccio consente di catturare relazioni ancora più complesse tra gli attributi, ma può rendere l'interpretazione del modello più difficile.

In generale, una funzione non lineare può essere rappresentata come:
\[
f_m(x) = g_m(x) > 0
\]
dove $g_m(x)$ è una funzione non lineare definita sui dati di input. L'uso di funzioni non lineari nei nodi può migliorare la capacità del modello di adattarsi a dati complessi, ma richiede anche tecniche di ottimizzazione più sofisticate per addestrare l'albero decisionale e non finire in overfitting.