\chapter{I primi modelli di Machine Learning}

\section{Neurone computazionale - modello di McCulloch-Pitt}
L'idea iniziale era quella di replicare la struttura di un neurone biologico con un modello matematico, in modo da poter simulare il funzionamento del cervello umano. 

\paragraph{Neurone biologico.} Un neurone biologico è costituito da:
\begin{itemize}
	\item Dendriti: ricevono segnali da altri neuroni.
	\item Corpo cellulare: elabora i segnali ricevuti.
	\item Assone: trasmette il segnale elaborato ad altri neuroni.
\end{itemize}

\noindent
Il primo modello di neurone computazionale fu proposto da McCulloch e Pitts nel 1943. Si basa su tre componenti principali: 

\begin{itemize}
	\item Degli input $x_1, x_2, \dots, x_d$ binari, che possono essere \textbf{eccitatori }e \textbf{inibitori}. 
	\item Una threshold $v$, una soglia
	\item Un output binario.
\end{itemize}

\begin{figure}[tbph]
	\centering
	\includegraphics[width=\linewidth]{./images/neurone_computazionale.pdf}
	\caption{Modello di McCulloch Pitt e due computazioni possibili: AND logico e OR logico}
	\label{fig:neuronecomputazionale}
\end{figure}

Supponiamo che $x_1, \dots, x_j$ siano eccitatori, $x_{j+1}, \dots, x_d$ sono inibitori. Se $j \geq 1$ e almeno un inibitore è $=1$, allora il neurone ritorna 0. Un inibitore è sufficiente per bloccare l'output. Altrimenti, si calcola $z = x_1 + \dots + x_j = x_1 + \dots + x_d$, ossia la somma di tutti gli input\footnote{posso considerare anche gli input degli inibitori in quanto $=0$.}. Se la somma è $\geq v$, allora l'output sarà 1, altrimenti 0.

Questo modello è tale da poter computare un $AND$ ($v=n$ e $n$ input eccitatori), un $OR$ ($v=1$ e $n$ input eccitatori, vedere figura \ref{fig:neuronecomputazionale}), ma non uno $XOR$!

\subsection{Limitazioni del modello di McCulloch-Pitt}
\begin{itemize}
	\item Non esiste un modo automatico di fare training.
	\item Gli input sono binari.
	\item Gli input hanno tutti lo stesso peso.
	\item Tutte le funzioni computabili sono linearmente separabili.
\end{itemize}

\section{Percettrone - Rosenblatt}

Il successore del modello di McCulloch-Pitt è il \textbf{percettrone}. Questo modello risolve il problema dei pesi, assegnando ad ogni ingresso un peso differente, e introduce una procedura di apprendimento automatico per stabilire i pesi in maniera opportuna.
\begin{figure}[tbph]
	\centering
	\includegraphics[width=\linewidth]{./images/percettrone.pdf}
	\caption{Modello di Rosenblatt, noto come Percettrone}
	\label{fig:percettrone}
\end{figure}

\noindent
Le componenti sono:
\begin{itemize}
	\item Features $x_1, x_2, \dots, x_d$ normalizzate in $[0, 1]$. Ciascuno di questi input ha associato un peso $\vartheta_1, \dots, \vartheta_d$.
	\item Una threshold $\vartheta_0$, una soglia.
	\item Un output binario.
	\item Una \textbf{procedura di learning automatico} per stabilire i parametri (il peso di ciascun input).
\end{itemize}

\noindent
Il comportamento del percettrone sarà il seguente:

$$
f(x_1, \dots, x_d) = 
\begin{cases}
	1 \qquad \displaystyle\sum_{j=1} x_j v_j \geq \vartheta_0\\
	0 \qquad \text{altrimenti}
\end{cases}
$$

Per semplificare la computazione, verrà aggiunta una feature $x_0 = 1$ con peso $\vartheta_0$, uguale alla threshold. Questo ci aiuterà a scrivere la funzione con la seguente notazione:
$$
f(x_1, \dots, x_d) = 
\begin{cases}
	1 \qquad \displaystyle\sum_{j=1} x_j v_j \geq 0\\
	0 \qquad \text{altrimenti}
\end{cases}
$$

\noindent
E poi come prodotto matriciale:
$$
f(x) = [\vartheta^T \overline{x} > 0]
$$

\noindent
dove $\vartheta^T$ è il vettore dei pesi. Supponiamo ora di avere 2 parametri, andremo a ottenere:
$$
\vartheta_0 + x_1\vartheta_1 + x_2\vartheta_2 \geq 0
$$

e con dei semplici passaggi, possiamo tracciare una retta nello spazio, chiamata \textbf{decision boundary}, che dividerà lo spazio in due parti, quella per cui $f(x_1, \dots, x_d) = 1$, e quella per cui $f(x_1, \dots, x_d) = 0$
$$
x_2 = x_1\frac{\vartheta_1}{\vartheta_2} + \frac{\vartheta_0}{\vartheta_2}
$$

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{./images/decision_boundary.png}
	\caption{Decision boundary del percettrone in 2D. I punti arancioni sono classificati come 1, quelli blu come 0 e la retta al centro è la \emph{decision boundary}.}
	\label{fig:decisionboundary}
\end{figure}

Questa retta, in un task di classificazione, suddivide lo spazio in due classi. In uno spazio 2D è una retta, in uno spazio 3D un piano. Lo spazio dovrà essere \textbf{linearmente separabile}.

\subsection{Processo generale di training del percettrone}
Descriviamo la procedura di training nel seguente modo:

\begin{enumerate}
	\item Inizializzazione casuale dei pesi $\vartheta_1, \dots, \vartheta_d \in \mathbb{R}$.
	\item Computa $\forall x^{(i)}$ il valore di $\hat{y}^{(i)}$.
	\item Confronta $\hat{y}^{(i)}$ con $y^{(i)}$
	\begin{itemize}
		\item Se  $\hat{y}^{(i)} = y^{(i)}$: non fare nulla.
		\item Se $\hat{y}^{(i)} \neq y^{(i)}$: vanno aggiornati i pesi. Analizziamo come modificare i pesi, in funzione dei risultati.
	\end{itemize}
	
	\item Aggiornamento dei pesi:
	
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			$\hat{y}^{(i)}$ & $y^{(i)}$ & Cosa fare\\
			\hline
			0 & 0 & ok!\\
			0 & 1 & riduci i pesi.\\
			1 & 0 & aumenta i pesi.\\
			1 & 1 & ok!\\
			\hline
		\end{tabular}
	\end{center}
	
	Questo sposterà la retta in maniera opportuna, convergendo ad un risultato opportuno per il dataset di training.
\end{enumerate}

Questa descrizione generale riflette a grandi linee il processo, tuttavia è opportuno conoscere la procedura dal punto di vista matematico, che riflette poi l'implementazione effettiva.

\subsection{Implementazione del training}
L'aggiornamento dei pesi avviene nel seguente modo:
$$
\underbrace{\hat{\vartheta}_j}_\text{nuovo peso}\leftarrow \vartheta_j + ( y^{(i)} -\hat{y}^{(i)})x_j
$$

Chiaramente, il percettrone non dovrà modificare i suoi pesi se $\hat{y}^{(i)} = y^{(i)}$. Questa formula lo contempla, in quanto $y^{(i)} - \hat{y}^{(i)} = 0$: il peso non verrà aggiornato. 
Si può aggiungere un parametro $\alpha$, detto \textbf{learning rate}. Il \underline{percettrone standard non lo prevede}.

$$
\hat{\vartheta}_j \leftarrow \vartheta_j + \alpha( y^{(i)} -\hat{y}^{(i)})x_j
$$

\noindent
Scelto in maniera opportuna, potrebbe migliorare l'apprendimento e la convergenza.

\subsection{Versione di Adaline}

Una versione successiva dell'algoritmo di ricalcolo dei pesi, per stabilire con maggiore precisione di quanto incrementare o decrementare i pesi, stabilisce che

$$
\hat{\vartheta}_j \leftarrow \vartheta_j + \left( y^{(i)} - \sum_{i}\vartheta_i \cdot x_i\right)x_j
$$

\noindent
Osserviamo che, per ogni esempio $i$-esimo, l'uscita desiderata $y^{(i)}$ può assumere solo due valori, $0$ oppure $1$. Allo stesso tempo, la somma pesata 

\[
\sum_i \vartheta_i \cdot x_i
\]

\noindent
rappresenta una combinazione lineare normalizzata degli ingressi, e pertanto assume valori compresi nell'intervallo $[0,1]$.  Ne consegue che la differenza tra il valore target $y^{(i)}$ e la somma pesata non potrà che appartenere all’intervallo $[-1,1]$. In altre parole, se il neurone commette l’errore massimo possibile, questo sarà pari a $1$ in valore assoluto, cioè la distanza più grande tra un’uscita binaria ($0$ o $1$) e un valore previsto all’interno di $[0,1]$.

\section{Modelli lineari}
Il percettrone funziona funziona correttamente solo se i dati sono \emph{linearmente separabili}, ovvero se esiste una frontiera lineare che separa le due classi. Ad esempio, il problema AND è linearmente separabile.

\subsection{Problema della separabilità non lineare}
\textit{Questa parte è un approfondimento teorico sulla separabilità lineare. Può essere saltata senza problemi.}

\paragraph{Definizione (separabilità lineare).}
Sia $X=\{x^{(1)},\ldots,x^{(m)}\}\subset\mathbb{R}^{d}$ e sia $y\in\{0,1\}^{m}$ un insieme di etichette.
Diciamo che i due insiemi di punti $A=\{x^{(i)}:y^{(i)}=1\}$ e $B=\{x^{(i)}:y^{(i)}=0\}$ sono \emph{linearmente separabili} se esistono $w\in\mathbb{R}^{d}$ e $b\in\mathbb{R}$ tali che
\[
w^\top x \;>\; b \quad \forall x\in A,
\qquad
w^\top x \;<\; b \quad \forall x\in B.
\]
Equivalente: esiste un iperpiano $w^\top x=b$ la cui parte positiva contiene tutti i punti di $A$ e la parte negativa tutti i punti di $B$.

\paragraph{Quante funzioni booleane sono linearmente separabili?}
Per $m$ argomenti binari esistono $2^{2^{m}}$ funzioni booleane possibili; solo una parte è realizzabile con un singolo classificatore lineare.
Dati noti:
\[
\begin{array}{lcl}
m=2 &\Rightarrow& 14 \text{ su } 16 \text{ sono separabili linearmente},\\[2pt]
m=3 &\Rightarrow& 104 \text{ su } 256 \text{ sono separabili linearmente},\\[2pt]
m=4 &\Rightarrow& 1882 \text{ su } 65536 \text{ sono separabili linearmente}.
\end{array}
\]
Non esiste una formula chiusa semplice che dia, in funzione di $m$, quante funzioni sono linearmente separabili; tuttavia si osserva che, all’aumentare di $m$, la \emph{frazione} di funzioni separabili decresce rapidamente.

\paragraph{Dicotomie di un insieme di punti.}
Dato $X=\{x^{(1)},\ldots,x^{(m)}\}\subset\mathbb{R}^{d}$, le possibili etichettature (o \emph{dicotomie}) sono $2^{m}$:
\[
\mathcal{Y}=\{0,1\}^{m} .
\]
Solo una parte di queste dicotomie è realizzabile mediante una frontiera lineare. Con $d$ fissata, il numero di dicotomie realizzabili cresce in modo polinomiale in $m$ (ordine al piu $m^{d}$), mentre il totale cresce esponenzialmente ($2^{m}$); di conseguenza,
\[
\Pr\{\text{dicotomia separabile linearmente}\}\;\longrightarrow\;0
\quad\text{quando } m \gg d .
\]

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\linewidth]{images/non_linear_separation-probability.png}
	\caption{Separabilità non lineare. A sinistra: andamento qualitativo della probabilità che una dicotomia sia linearmente separabile al crescere del numero di campioni \(m\) (con dimensione \(d\) fissata); a destra: esempio in \(\mathbb{R}^2\) di punti non separabili con un singolo iperpiano.}
	\label{fig:nonlinear-separability}
\end{figure}

Intuitivamente: se il numero di campioni cresce molto a parità di dimensione $d$, è sempre meno probabile che una separazione perfetta con una sola retta/iperpiano esista.

\subsection{Problema dello XOR}
Supponiamo però di voler computare lo XOR logico. Per farlo, costruiamo la tabella: 
\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		\(x_1\) & \(x_2\) & \(XOR(x_1,x_2)\)\\
		\hline
		0 & 0 & 0\\
		0 & 1 & 1\\
		1 & 0 & 1\\
		1 & 1 & 0\\
		\hline
	\end{tabular}
\end{center}

\noindent
Il problema XOR non è linearmente separabile (immagine \ref{fig:xor-schema}): i quattro punti \((0,0),(1,0),(0,1),(1,1)\) con etichetta di uscita \(0,1,1,0\) non possono essere separati da una singola frontiera lineare. Un percettrone (che produce solo confini di decisione lineari) non riesce quindi a risolverlo.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.55\linewidth]{images/xor-schema.jpg}
	\caption{Schema del problema XOR: i vertici \((0,1)\) e \((1,0)\) appartengono alla classe \(1\), \((0,0)\) e \((1,1)\) alla classe \(0\). Nessuna frontiera lineare può separare le classi; è necessaria una frontiera non lineare (ottenibile con uno strato nascosto).}
	\label{fig:xor-schema}
\end{figure}

\paragraph{Dimostrazione della non-linearità dello XOR.} Assumendo la tabella di verità dello XOR e un percettrone con pesi \(v_1,v_2\) e soglia \(v_0\), le condizioni di attivazione diventano

\[
\begin{aligned}
	&1\cdot v_1 + 0\cdot v_2 > v_0,\\
	&1\cdot v_1 + 1\cdot v_2 < v_0,\\
	&0\cdot v_1 + 1\cdot v_2 > v_0,\\
	&0\cdot v_1 + 0\cdot v_2 < v_0.
\end{aligned}
\]

Da cui si ottiene
\[
2\,v_0 < v_1 + v_2 < v_0,
\]
ovvero una contraddizione: non esiste configurazione dei parametri di un singolo percettrone che risolva XOR.

\paragraph{Rete di percettroni.} La soluzione consiste nell’usare una \emph{rete di percettroni} (uno strato nascosto) che trasformi lo spazio in modo da rendere il problema linearmente separabile all’uscita. Un esempio minimale è:
\[
\begin{aligned}
	&f_1(x_1,x_2) = [\,x_1 - x_2 \ge 0.5\,],\\
	&f_2(x_1,x_2) = [\,x_2 - x_1 \ge 0.5\,],\\
	&f_3\big(f_1,f_2\big) = [\,1\cdot f_1 + 1\cdot f_2 \ge 0.5\,],
\end{aligned}
\]
dove \([\cdot]\) indica una funzione soglia (vale \(1\) se la condizione è vera, \(0\) altrimenti). Le funzioni \(f_1\) e \(f_2\) realizzano una trasformazione non lineare degli input; nello spazio così trasformato l’uscita \(f_3\) è ottenuta tramite una semplice soglia lineare.

\begin{figure}[tbph]
	\centering
	\includegraphics[width=0.7\linewidth]{images/neural-net-xor}
	\caption{Rete di percettroni per computare lo $XOR$}
\end{figure}


Infine, sebbene reti di percettroni possano risolvere $XOR$, l’algoritmo di apprendimento del singolo percettrone non è direttamente applicabile a reti multistrato; storicamente ciò contribuì al primo \emph{AI winter} e la situazione migliorò con la formalizzazione della \emph{retropropagazione} (\emph{backpropagation}). 
